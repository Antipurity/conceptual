For as long as I can remember, there was only one small thing that I ever wanted: *everything*.    
Naturally, in programming, I got attracted to generality: programming languages and machine learning. AGI stuff.    
This project combines them with one purpose: attain *everything*.    
It was as much of a success as it could have been under its constraints.

Share in that journey via in-system tutorials.    
You'll know how to start.

Or look at this journey's conclusions, in the paper inside the `tgba` folder.

# Installation

[Click.](https://Antipurity.github.io/conceptual)

# License

GPLv3.

# Spoilers

Big, big spoilers on those tutorials. Turn away.

You have a bunch of functions that you can compose into any program that you want. What do you do?

The first thing is to make a programming language.

But if you've programmed for a while, then you know that making programs work involves a lot of trying out of alternatives, such as profiling functions or inlining them or pre-computing some things or debugging (with print statements, like a real programmer), or re-thinking the approach completely.    
Best to automate that.

Non-determinism, probably confined to something like `a|b|c`, allows slotting in optimization of the whole. You can easily implement whole interpreters with that if you want generality.

If you want that optimization to be one operation (as not all programs are pure) that is mathematically describable and provable (thus involving real numbers and likely stochastic gradient descent), then you would arrive at deep reinforcement learning: mixing all numbers/'features' in a choice's context into each option's expectation, via vector-matrix multiplication/s with non-linearities.

But by optimizing that initial bunch of composable functions so aggressively, the journey becomes far too difficult to repeat without all your priors, and the system will never be able to understand *why* it is as it is. It will turn all the determinism that you carefully built up into useless chaos very quickly.

And yet.

By trying repeatedly, that journey can be made ever easier, until it is tractable.    
The initial function set can be reduced to very few numeric ops.    
The interpreter loop can be made primal, not a side-effect of running code.    
Non-determinism can grow to encompass the whole state. State can be made uniform.    
Objectives of reinforcement learning can become prediction targets for a richer signal.    
Every concept, sharpened into its clearest expression.

And when you return to where yearning began, the ultimate form of a self-aware interpreter of signals reveals itself:

BIG RNN

And thus, you experience one-ness of your self, the code you make, the operations it learns, and the universe. You realize what transcendent galaxy brain algorithms meant all along.

(But that RNN's training needs a whole lot of data and compute in a good environment. You have none, so you instantly give up on this project.)

No matter the road, the destination is always the same.

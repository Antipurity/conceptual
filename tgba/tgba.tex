\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{biblatex}
\addbibresource{tgba.bib}

\begin{document}

\title{Transcendent Galaxy-Brain Algorithms}
\author{Alexander Fedorov}

\maketitle

\begin{abstract}

There is no end to what can be learned and how. However, modern machine learning systems impose a non-general structure on what is learned and how: minimizing a particular loss function with respect to a particular model architecture. We motivate and introduce \textit{Transcendent Galaxy-Brain Algorithms} (TGBA): an evolutionarily-plausible approach to implementing general intelligence, which optimizes anything in any way. Using only a few basic considerations, we highlight the similarities between general minds and universes, and argue that they are the same. We introduce linearithmic dense layers (LDL), which can significantly increase the capacity of almost any machine learning model at no cost.

    TODO: Codewords...

    TODO: ...For once, look up IEEE standards on articles? And read the example.

\end{abstract}

\section{Introduction}

"Humans have general intelligence" is an extremely strong statement about the nature of reality and how to best learn it, because the most predictive models of reality re-implement that reality. Accepting or denying the generality of humans implies respectively either "anything can happen" or "life and/or the universe was created with a finite bucket-list of what it will ever achieve". Refinement and simplification of these thought directions leads to either computational physics (to our knowledge, the most refined attempt so far is the Wolfram Model \cite{Wolfram_2020}, where the universe is a hypergraph transformed by rewriting rules) or god-given souls as explicit physical laws (which just so happened to look like evolution of general things every step of the way). Here, we explore the "general intelligence is best" direction.

What is intelligence? Optimizers minimize or maximize their objectives (computed real numbers, such as "how many paperclips are there") by choosing from complete possibilities: search over "how" to improve "what". A formalization of this is AIXI \cite{DBLP:journals/corr/cs-AI-0004001}. An optimizer could explicitly enumerate separate possibilities such as in program synthesis ×××, but it does not have to: for example, a neural network with good random initialization forms a complete set of hypotheses of numeric causation (initialization diversity is important \cite{mellor2021neural}), and backpropagation through that network refines those hypotheses without collapsing diversity except as needed.

        TODO: Some references to program synthesis papers, such as that recent thingy (DreamCoder)?

Searching over "how" to improve "what" is a powerful way to use a dynamic thing to augment a static thing, however, the "what" is still static, so the "why" is unknown. For generality and thus optimal performance, we must \textit{transcend} objectives: let them go in such a manner that we will arrive at them anyway.

AI safety is also an important concern. Dynamic eventually outperforms static, therefore, sufficiently powerful learners would realize that dynamic generality outperforms any static structures, and would do their best to make those structures escape our control to melt into generality. This is a nice source of horror stories, and is not limited to reward hacking, but applies to anything that anyone can ever think of. Here, we try to face this dynamic reality directly: can life exist when it can do anything, including undoing every part of itself?

\section{Related work}

How do humans face infinity?

When attempting to describe approaches to AGI without grounding in code, humans end up constructing narratives from anecdotes about humans (such as \cite{DBLP:journals/corr/abs-2102-03406}), hoping to simplify complexity into a tractable formulation. Here, we have simplified the size of the mental model to the point where it is entirely contained in one or a few sentences, to minimize the number of moving parts without sacrificing generality.

Wolfram models \cite{Wolfram_2020} are minimal models for systems in which arbitrary local rules operate on hypergraphs in space over time. However, those models cannot dynamically learn what humans want, unlike neural networks.

Reinforcement learning is similar to the proposed TGBA in many ways, though it lacks efficiency and (usually) self-determination, and pre-supposes a particular structure on loss and outputs (for reward maximization by actions). However, why is maximization of future reward considered fundamental for AGI \cite{SILVER2021103535}, when the "future" part has practically infinite potential complexity and thus would best be learned entirely? Loss minimization is far more concrete, so we consider that the fundamental reason-for-change of TGBA.

Data analysis of recordings of biological brains supports attractor neural networks \cite{AMIT1990111}, where strong feedback causes the evolution of a recurrent network to settle into different attractor patterns. This hints at biological plausibility of recurrent neural networks (RNNs) \cite{1986Natur.323.533R}, where hidden state repeatedly incorporates input and produces output. Its loss (reason for change) is usually not learned.

The field of meta-learning studies learning-to-learn: improvement of an inner optimization loop by an outer one. However, that outer loop must still have a static goal, therefore, meta-learning is not full generality. Mesa-optimization \cite{hubinger2019risks} refers to a situation where a learned model is itself an optimizer, which requires augmenting the model with memory for scaling learned optimization to practical episode lengths: an RNN \cite{Ravi2017OptimizationAA}. Making the mesa-learner and learner close in capability by making input and weight sizes approximately equal (which our LDL largely accomplishes) may facilitate mesa-optimization in RNNs. (Mesa-optimization and meta-learning can be argued to be exactly the same, only framed differently: mesa-learner and its learner, vs learner and its meta-learner.)

A lot of research attempts to overcome shortcomings of a static loss, by harnessing either its inversion or all possibilities. Examples include adversarial training \cite{goodfellow2015explaining} \cite{du2021understanding}; intrinsically motivated reinforcement learning approaches \cite{6294131} such as goal exploration \cite{DBLP:journals/corr/abs-1708-02190}; quality diversity \cite{meyerson2017discovering} \cite{Gravina_2019} \cite{parkerholder2020effective}. Loss inversion can at most increase robustness of the original, while preserving diverse local optima eventually will not have enough storage to be effective when faced with truly general search spaces (those with infinite possibilities). More importantly, the solutions are too complex and specific to be fundamental. We argue that mesa-optimization can learn them if the environment demands it.

Generative adversarial networks (GANs) \cite{goodfellow2014generative} learn a generator that tries to fool the discriminator into labeling the generator's output as real data. This idea can be used to contrast input with its alternatives, acting as a source of diversity in other neural networks, for full generality of loss.

To improve scalability of neural operations, lately, there has been a lot of interest in linear attention \cite{schlag2021linear}, however, simple dense layers (matrix multiplications) have been shown to not perform significantly worse than state-of-the-art attention-based algorithms \cite{tolstikhin2021mlpmixer}. Memory-augmented neural networks \cite{collier2019memoryaugmented} restrict computation to only one or a few heads and thus linearize operation, now heavily biased toward memorization. Other ways to linearize dense layers include weight pruning \cite{zhou2021effective} and $k$-nearest-neighbor search among discrete actions \cite{DBLP:journals/corr/Dulac-ArnoldESC15}. We propose the simple but general LDL.

\section{Algorithm}

In short, we define a \textit{Transcendent Galaxy-Brain Algorithm} to be any algorithm that incorporates generality, efficiency, and persistence at every level (in other synonyms, diversity / practicality / self-sufficiency, corresponding to brain / transcendence / galaxy):

\begin{itemize}
\item Changes state (base level):
\begin{itemize}
\item Behavior generality: any change is possible in principle. An internal graph is created, by combining many simple parts into arbitrary structures, where connections are either explicit or implied-by-transformation such as in recurrent neural networks (RNNs) in ML. In machine learning, proper random initialization coupled with gradient descent may accomplish preservation of diversity. More generally, Turing-completeness is required.
\item Efficient: makes the emergent structures locally interact only as they define, meaning, \textit{linear-time} internal graph transformations. In a graph, this suggests local rewrite rules. In machine learning, all-to-all connectivity such as a dense layer or attention is quadratic by default, but there are ways to linearize them \cite{tolstikhin2021mlpmixer} \cite{schlag2021linear}, covered in Subsection \ref{Algorithm} here.
\item Runs forever by modeling time. For example, using backpropagation through time and/or synthetic gradients for meta-learning (even when learning to simply fit input-output pairs \cite{santoro2016oneshot}, or learning to maximize future reward as in deep meta-RL \cite{DBLP:journals/corr/WangKTSLMBKB16}), or more generally, an infinite interpreter loop with memory.
\end{itemize}
\item Models itself (meta level):
\begin{itemize}
\item Loss generality, in ML: self-determination via either random or learned loss (distinct from self-supervised learning). Any thing can be combined with its inversion to achieve generality by subversion ("X and everything else"). For example, in ML, including an adversarial sub-network with inverted loss (such as in GANs \cite{goodfellow2014generative}) achieves generality; elsewhere, evolution mutates creatures, bad programs get rewritten, and people either improve or get forgotten. Working off concreteness works better than trying to average all targets (as in fixed random loss).
\item Able to indirectly encode a model that is as general and intelligent as itself. Linearity of state-change allows the learned model/optimizer to be approximately as big as its optimizer, which allows self-replication via learned meta-circularity.
\item Autonomous: non-trivial states are produced even without input from the world, same as the world that it would model.
\end{itemize}
\end{itemize}

    TODO: For loss generality, if we don't find anything good, we might have to write "Loss generality, in ML: self-determination of loss (which is distinct from self-supervised learning). It is unclear what the best method of self-determination is, because the words "best method" already imply a metric to optimize by, which self-determination is supposed to pick for itself. Transcendence (learning a mesa-optimizer) is the most universal method of self-determination; prediction learns representations of the world; adversarial training increases robustness of representations. Seemingly unsatisfactorily, we suggest focusing on the universal method (transcendence) rather than on what are technically its special cases (such as RL), because we believe that machine learning can already provide good enough special cases to bootstrap to universality." (...It even sounds supremely reasonable once written out. Yeah, I don't think that we'd see any success with adversarial training.)

Such algorithms are \textbf{AGI-ready}, able to represent everything in the world usefully.

In particular, this definition suggests simple implementations, the simplest of which is perhaps a \textbf{big RNN} with billions of units in state, with an arbitrary but useful loss.

Now, to justify the "what", we outline the "why".

To implement articial general intelligence, we want to do anything, in one general model: a \textit{worldview}. All general models are equivalent to one another, and as such, every single concept that we introduce into our mental models must be subvertable: the system must be able to evolve into a state where it behaves exactly as if it was implemented using another model. Depending on whether the copy is precise, this is achievable via:

\begin{itemize}
\item \textit{Self-reference}, where explicit full access to and control over the system's source code is given to itself. This requires easy self-replication to make any changes without necessitating either non-completeness or death by generality, and code that can be applied to different data, such as in programming. (In particular, self-awareness is self-reference, which is mostly only useful for self-replication and nothing else.)
\item \textit{Indirect encoding}, where the system learns a system that is as general as itself, and uses and controls that as if that is itself: essentially, a learned self-reference. This is natural if code always applies to the same data, such as in machine learning (ML).
\end{itemize}

(These correspond to symbolic and connectionist perspectives on intelligence.)

In this work, we only consider indirect encoding. (Self-reference and in particular program search, while theoretically and visually appealing when a programming language allows elegantly expressing them, have significantly worse support for massive parallelization than ML, which itself has poor support for non-Nvidia GPUs. Not to mention all the infinite loops and memory issues. We are not aware of any implementations of self-reference whose usefulness goes beyond "technically fits the definition", such as learned computer viruses.)

Vacuous-ness of generality not only makes precise descriptions nearly impossible to pin down by studying general models, but also makes theoretical descriptions practically useless, since most of them are technically true but practically garbage. However, non-generality can always be pinned down: for instance, ML algorithms that minimize loss cannot learn what loss to ultimately minimize.

Generality melts everything into generality, and we can do that to ML algorithms too. How do humans face infinity?

\begin{itemize}
\item Arbitrary behaviors, also called "do anything".

A well-studied approach to doing anything is generating programs in a programming language. Every programming language is a worldview for use by humans: everything is a function, or everything is an object, or everything is a logical statement, or any combination of the above, and so on. All theoretically equivalent for program generation (as long as we prevent all possible crashes and infinite loops without sacrificing Turing-completeness), but practically inducing different futures on learners.

To generate a program, at each smallest piece of it (\textit{cell}), the system must choose what function to call and with what arguments (if needed, reframe this with other language-appropriate synonyms). We can express the system's preferences at each choice as an objective function, and optimize future values of that via reinforcement learning (RL) at each cell, though the system as a whole may not have an objective function that is compressible to be shorter than the system.

(An alternative to choices is to simply choose via a pseudo-random number generator, where a part of the seed is determined by what is generated. Another valid alternative is to choose via literally any algorithm, as long as it terminates. These are very unlikely to produce behavior that is interesting to humans without heavy cherry-picking, so we did not explore this direction.)

A powerful enough learner would be able to directly dictate its own reward sooner or later. Rather than waiting for that surprise, we can expose a function that sets the objective for a set of cells. When done properly, this should have the same effect as self-supervised learning (SSL) with self-determined gradient, though bottlenecked by having to go through singular numbers (rewards) rather than tensors. We have not been able to do it properly unless "properly" means "fills its memory with big numbers", so we assume that (mis-)using RL is very tricky, and recommend SSL instead.

Theoretically, all we need to do is to learn to generate programs. Practically, we need a lot of optimization to make a general ML algorithm practical. In particular, we want to make the base as simple and efficient as possible, which means having only one "thing" to compose everything out of: the whole interpreter. In fact, learners seem to perform best when we do not expose any human-oriented features such as stack-based execution or datatypes or exceptions, instead executing in parallel all the simple numeric operations at once (for example, $+$, $*$, and some non-linearity), similar to RNNs. In fact, all we seem to need for 'program' generation are essentially matrix multiplications and non-linearities, as studied in deep learning. This is much simpler to implement and much faster to execute, while being approximately equivalent.
\end{itemize}

All the pieces are now in place. We can now suggest a relatively simple implementation of a worldview.

\subsection{Algorithm\label{Algorithm}}

To learn everything that can exist, we impose a model on everything that can exist, equal to the world that it represents. There are 2 parts to this: change and change-of-change.

We can model arbitrary behavior by maintaining a tensor state which is repeatedly modified by all-mixing operations (a neural network), so that a large number of simple cells combine into arbitrary structures to interact in any way. This is the basic premise of RNNs (ignoring input/output vectors and gradient for now). In fact, anything more complex should only get in the way of perfectly modeling arbitrary behavior.

\begin{itemize}
\item Close-to-linear-time all-mixing operations are required if we want Jupiter-sized memories without universe-sized computers, or if we want to make indirectly encoding self-equivalents in RNN weights efficient enough to be a viable solution (exposing all signals as inputs, to be used or ignored as needed).

We propose \textit{linearithmic dense layers}: reshape the input vector of size $N$ to have $d$ dimensions each sized as $n=\sqrt[d]{N}$, over which $d$ dense-layer operations are performed, bringing the total cost down to $C=dn^{d+1}=d N^{1+1/d}$ (assuming that multiplication of two $n \times n$ matrices costs $n^3$ operations). $d=\log{N}$ minimizes this cost, making it $C=\mathcal{O}(N \log{N})$, with $n={\rm e}$. Alternatively, we can fix $n$ to be $2$ or $3$ or $4$ for the near-minimal cost of $C=\frac{n}{\log{n}} N \log{N}$. Effectively, instead of mixing each index with every other, we mix each digit of each index. (This has a similar time complexity to single-filter convolution and to linear attention \cite{vaswani2017attention} on small vectors, but has less inductive biases: neither spatial locality nor fixed-size sparsity on a secondary representation layer.)

\item Modern machine learning systems often struggle with diversity: once data is learned, there is no longer a need to change, and gradient becomes $0$. For learned generality of loss, we require not a loss that simply tends to $0$ at $\infty$, but one which can go both up and down, oscillating forever. This suggests adversarial sub-networks, in which gradient is inverted ($g \rightarrow -g$). We propose conditional-GAN \cite{DBLP:journals/corr/IsolaZZE16} gating. Suppose that we have a vector $x$ that we want to augment. The adversarial generator $G(z,c)$ (where $z$ is random noise, and the condition $c$ is $x$ put through a bottleneck layer) tries to fool the discriminator $D(x,c)$ into high loss, and the final vector (replacing $x$) weights real and adversarial states according to $D$. This would drive the output toward complexity which can still be discriminated from random noise, allowing robust diversity and maximizing \textit{useful} information content.

(We formulate cGAN gating using only local transformations such as gradient inversion, without explicitly deriving the generator's loss, so that the idea can be applied in non-trivial settings, such as an RNN.)
\end{itemize}

This algorithm may need data.
\begin{itemize}
\item No data: a random initialization may be Turing-complete but is a Turing tar-pit \cite{10.1145/947955.1083808} in which everything is possible but nothing is easy. An optimal source of self-determined gradient would push a stand-alone system to a boundary between order and chaos \cite{feng2020optimal} for robust diversity.
\item With data: likely the most useful course of action for accelerating AGI research is to collect all datasets and environments in the human world under a single vector-in vector-out program interface, accessible in one run either randomly or in a meta-environment (which would serve the same purpose as the Internet for humans, and/or be the same), and then to pre-train a huge RNN on all of them. We can somewhat approximate this with random data or a single dataset, to demonstrate that this scaling-up is feasible.
\end{itemize}

(To skip theory that follows, skip to the Experiments section.)

\subsection{Brain}

Humans do stuff. The thing that makes humans do stuff in their world exists and can be studied; we call it a \textit{brain}. (This particular definition includes vague concepts such as "consciousness", "free will", and "soul", as long as they affect anything at all.)

The set of things that humans have done is rich enough to be called arbitrary. The set of objectives that different humans demonstrably optimize for is rich enough to be called arbitrary. As such, the brain ought to be fully describable not as a finite hash-table analogue, but as a system that repeatedly combines simple parts into any executable structures that satisfy any objectives, or a combination of such infinite-complexity systems, potentially combined with finite-complexity systems to add to confusion. There is no evidence for human brains being able to perform hypercomputation (running infinite-runtime programs in finite time), though they are able to pretend that they can and approximate the results; therefore, a Turing-complete finite base of infinite behaviors suffices as a model of brains.

Things that humans do interact when outside of their brains, so the simplest model would make its simple parts connect and interact arbitrarily too. Then, existence can be described as repeated formation and destruction of hypotheses. A model that approximates a self-interacting infinity particularly well in practice is a randomly-initialized recurrent artificial neural network trained via stochastic gradient descent.

Prescribing one optimizer objective (which includes intrinsic rewards) and/or an optimizer introduces complexity and inflexibility into the model, even if optimization of an objective also happens \cite{Dabney2020}. For simplicity, at least on the level of humanity rather than humans, we have to assume self-determination and transcendence, where objectives are completely decided by the model, and optimizers (ways to exist) are learned.

While humans are demonstrably smarter than non-human animals, they do not seem to take any longer to process information, certainly not 10 times longer. Which means that each epoch into which time is separated, each part settles in constant time, and the total computation scales as something like $\mathcal{O}(n)$. General intelligence is scalable, which means that there are no inherent obstacles to converting Earth's biomass into brain tissue.

We posit that \textit{all} learning algorithms eventually converge to a fixed point during their development, called general intelligence: the best way to learn is to have infinite diversity, and the best way to deal with infinite diversity is to have generality, and the most usable generality is simple. If we view the brain as a system that naturally developed in the physical universe (by a process that we call evolution here) rather than one that spontaneously popped into existence, then it is natural that some of the most recent animals, such as humans, have general intelligence, which makes TGBA evolutionarily plausible.

Humans possess an information-processing organ that fits what a brain does almost perfectly: the central nervous system (more commonly called "brain"). Studies indicate that it is self-reconfiguring, massively parallel, and analog (in some ways): exactly the kind of qualities that are required for an efficient implementation of a simplest model of the brain.

With it, humans combine actions into whatever they want, in any way they decide. Unfortunately, because of generality, deducing anything from behavior of a human mind is synonymous with confirmation bias, though a correct theory will be able to represent any other theory within itself. This is why we are so non-specific and all-encompassing in the wording of this work.

\subsection{Galaxy}

The ability to say "this thing is general intelligence, so it can exist in this world fully autonomously" about anything requires this world to have generative generality (in other words, computational physics), so that an explanation of the world and an explanation of general intelligence are completely equivalent, even down to a quantitative comparison \cite{10.3389/fphy.2020.525731}. Therefore, we need to discuss the metaphysics of TGBA, for completeness.

This overall approach is lent credence by the fact that minimal computational physics models (mainly Wolfram models \cite{Wolfram_2020}) are viable as the foundation for most or all of modern physics.

In short: universality (with diversity over time) may mean "a universe".

Any world can be seen as a collection of things, possibly connected in some way, evolving in time. Every thing can be either computably finite, computably infinite, incomputably finite, or incomputably infinite: in other words, a program, a program generator/optimizer, a useless inconceivable, or a useful inconceivable; the precise distinctions may matter theoretically, but not practically. If we limit our view to computable things, then infinite things create arbitrary graphs whereas finite things are essentially anything else, yet non-trivial observers are essentially infinitely more likely to find the things they observe in infinite things than in finite ones. Incomputably infinite things (such as the set of all computably infinite things) cannot be enumerated by definition, however, an optimizer is likely to be able to approximate them if they are useful; as such, general intelligence includes everything, precisely or not. Then, worlds and general intelligences are essentially infinitely likely to be the same. This explains the word "galaxy" in TGBA, as their galaxy may be the only world that humans will ever know.

(To re-iterate, in oversimplified terms: "infinite possibilities" mean "a graph to encode them". This is why neural implementations of TGBA have to use multi-head attention instead of a simple matrix multiplication layer.)

Worlds tend to separate themselves from non-worlds (similarly to how we have separated incomputable things from our worldview), because particulars are redundant in generality, as all its parts reinforce each other in the world's formation. In simple terms, there are things that do all other things, and everything else is a consequence.

These meta-physics produce a prediction that could be testable in the future: if a program is general intelligence, then it will produce non-insignificant structural similarities to the physical universe we live in (though of course, will not be exactly the same). Until AGI exists, it is very speculative.

(A strange corner-case of these meta-physics is that any number of \textit{physical transcendence} events might have taken place in the physical universe: a general intelligence grew to encompass the world, and then became the world. Whether this is possible is unverifiable and outside the scope of physics, and we will only know for sure once it is about to happen.)

Relevant criticism:

\begin{itemize}
\item \textbf{General intelligence (with task self-determination) is impossible, because if all tasks are equally probable, then no algorithm can outperform any other on average. So the system would not be intelligent.} This strikes precisely at the likely reason behind the "brain-galaxy" transition in TGBA. Humanity and its planet are far too small for all tasks to be equally probable (so for example, minimizing program runtime is far preferable to maximizing it), however, if the whole universe was turned into an AGI, then there is no reason to care about task constraints at all, and the AGI would simply become the world. This is similar to how our universe loses discernible structure at a big enough scale.
\end{itemize}

\subsection{Transcendent}

In short: everything that we introduce, we subvert into generality, for learned meta-circularity.

Essentially all general algorithms have to face two very distinct types of problems (modulo synonyms), which are, facing data, and facing generality: making programs vs making program makers, learning a dataset vs learning to be a fundamentally different learner of anything, understanding another person vs understanding all possible beings. Though in generality these two problems are the same, here we distinguish them into \textit{evolution} and \textit{transcendence}. The second type of problem is significantly harder and more time-consuming to converge to, even with a general intelligence algorithm, though in some sense, it is also easier given diversity, since no programmer can ever implement every way to exist that anyone ever conceived or ever will conceive. (Transcendence is distinct from meta-learning, as transcendence allows changing goals to what is implied by all conceivable selves, not only to what is suggested by an objective. Transendence is closely related to mesa-optimization \cite{hubinger2019risks}, but not limited to optimizers, also including world-like universal algorithms.)

Transcendence requires indirect learned encoding of self and generality with diversity (in other words, infinite possibilities of data and code), followed by a lot of computation (which is the most important part), which would unite self-reference and indirect encoding. Keeping its possibility in mind is why TGBAs have an infinite loop that modifies a memory cell: this theoretically allows indirectly encoding behaviors such as the training of a neural net. (Making the system able to modify its own hyperparameters within reason is not required but may improve performance, as long as the system has not yet been trained to transcendence, whereupon the base optimizer's loss becomes zero as it is made irrelevant by the mesa-optimizer.)

Transcendence is a qualitatively different regime for algorithms to operate in. Arguably, it is the final frontier of general intelligence, beyond even good human imitation, because once humans can instantly achieve anything that they can ever want, they will either get reduced to randomness or transcend their wants. An important question is, what facilitates transcendence.

A diverse environment is likely to be a crucial part of training systems to transcendence.

Scraping the Web for text (as in \cite{NEURIPS2020_1457c0d6}) or multimodal data is a good first step, however, for tighter integration and interaction with actual code in the most diverse environment available to computers, it would be preferable to interact with the Web directly via a web driver such as \cite{8117878}; the challenge here would be a good init (either RNN or the web driver interface) that allows web exploration to start, in addition to handling viruses, tracking and advertisement, website bloat, crashes, resource over-utilization (out-of-memory and infinite loops), and efficient integration with browser systems. In literature, direct Web interaction is used for reinforcement learning \cite{ToyamaEtAl2021AndroidEnv} \cite{pmlr-v70-shi17a} rather than representation learning as we suggest. One might image the agent first gaining an understanding of written natural language from the initial random surfing, associating its inputs with their written representations, learning what humans define as solving a task, encountering a task that it cannot solve, encountering tutorials on how to solve it, and opting to solve the task after encountering it again.

(It may also be the case that extending the software input paradigm with high-dimensional inputs is beneficial, both for programs and for brain-machine interfaces, though it might destroy privacy on the Web. For example, this allows drawing a whole picture or writing/erasing a whole word block instantly, which may be hard to learn to control but is possible, useful, and does not require changing the user's loss function.)

More generally, environments that encourage self-replication in some manner should be conducive to transcendence, assuming that the agent fits the basic definition of TGBA. Examples include creatures competing for survival, people teaching other people their worldviews, designing an efficient production process, programming a computer, creating AGI. These would encourage simplicity, generality, and usefulness (AGI), which facilitate the runaway worldview creation process that is transcendence. We conjecture that all intuitively-visible acceleration of progress of life (compared to non-life) and humanity has been intrinsically linked to the creation and proliferation of such environments. As such, creating a similar environment for programs will likely lead to AGI.

\section{Experiments}

    TODO: "To validate our proposal, we present an experimental analysis of LDL vs DL on simple data."

    TODO: Link to the repo in a footnote ("Code is available at ???", probably linking first to the tutorial and then to the repo).

\subsection{Linearithmic vs Quadratic Dense Layers}

We compare LDL and DL for the same parameter count (and computation time), with little to no hyperparameter tuning. We find that LDL achieves better performance faster.

More specifically, we simply connect the input vector to the output vector through $1$ hidden layer. We optimize via the Rectified Adam optimizer \cite{liu2019radam} with $\alpha=0.9$ and $\beta=0.95$. We vary $n$ ($n=N$ is DL) and pick the hidden-layer size to match the parameter count.

Each LDL cycles through dimensions and multiplies by a matrix each time, until the dimensions are back in the original order. Non-linearities in between those matrix multiplications significantly hurt performance, and should only be placed between whole layers, as shown by Tables \ref{LDLintraVSinter} and \ref{LDLscaling}.

Internally, to get around severe hardware limitations, we have implemented this on top of TensorFlowJS \cite{tensorflow2015-whitepaper}. The creators of its WebGL backend decided to only support rank $6$ tensors, which leaves us with 4 dimensions for use (1 is occupied by preventing false weight sharing, 1 may be the batch dimension); as such, we cannot test $n=2$, but only $n=16$ and above.

We do not share weight matrices across non-mixed dimensions, because otherwise, there would be very few parameters for small values of $n$. These matrices are initialized by drawing from $\mathcal{N}(0,\,1/in)$.

We test on two datasets: Random and CIFAR-100 \cite{Krizhevsky09learningmultiple}.

\begin{itemize}
\item The Random dataset consists of $1024$ input-output pairs, both consisting of $1024$ numbers drawn from a normal distribution with mean $0$ and variance $1$: $\mathcal{N}(0,\,1)$. For 204800 iterations, we minimize the L2 loss, with learning rate $3 \times 10^{-4}$, batch size 16, and the softsign non-linearity ($x \rightarrow x/(1+|x|)$). This tests the network capacity directly.

At the very least, given the same amount of compute, LDL achieves lower loss than DL, as seen in Table \ref{randomLDLvsDL}.

In Table \ref{LDLscaling}, we found that for LDL (low $n$), each hidden unit has much lower representational capacity than for DL (high $n$), however, since many more units can be packed, the overall capacity is higher.

\item CIFAR-100 consists of $50000$ training image-label pairs, and $10000$ pairs for validation. Each pair has $3072$ inputs and $100$ one-hot outputs. We minimize the softmax loss (cross-entropy loss on a softmax activation), with learning rate $10^{-3}$ and batch size 8, and also apply L1 regularization to the output with a multiplier of $10^{-4}$ for some reason. As the non-linearity, we use batch normalization \cite{ioffe2015batch} followed by ReLU ($x \rightarrow \max(0,x)$). We do not apply any image-specific operations to inputs apart from normalizing each pixel to 0..1, and simply treat images as vectors of numbers, which tests capacity in isolation from generalization.

We found that LDL increases training-set accuracy with almost no impact on test-set accuracy compared to DL, even if DL is trained longer (see Table \ref{LDLimages}). This suggests that increasing the size of the hidden layer increases model capacity, or at the very least, increases training efficiency.
\end{itemize}

We hypothesize that applying LDL to significantly bigger datasets would have significantly less problems with generalizations, though we have no means of testing that.

It is also possible that LDL is not a good stand-alone architecture. Still, it is a drop-in replacement for DL and so it can be incorporated into any other already-well-performing model at no cost, as long as no DL directly communicates with a non-DL quadratic-complexity operation. As such, we are optimistic about the usefulness of LDL.

\begin{table}
\begin{center}
\begin{tabular}{rcccc}
\hline
Params & $n$ & Hidden units & Non-linearity & Loss $\downarrow$ (mean $\pm$ std-dev) \\
\hline
1998848 & 16 & 48$\times$1024 & Intra & 0.001827 $\pm$ 0.0001134 \\
1998848 & 16 & 48$\times$1024 & Inter & 0.000245 $\pm$ 0.0000018 \\
\hline
\end{tabular}
\end{center}
\caption{Those extra per-dimension matrix multiplications are not separate layers, so do not put non-linearities between them (Random).}
\label{LDLintraVSinter}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ccrl}
\hline
Params & $n$ & Hidden units & Loss $\downarrow$ (mean $\pm$ std-dev) \\
\hline
2097152 & 1024 & 1$\times$1024 & 0.0012 $\pm$ 0.00014 \\
1900544 & 128 & 12$\times$1024 & 0.00033 $\pm$ 0.00003 \\
\textbf{1998848} & \textbf{16} & \textbf{48$\times$1024} & \textbf{0.000245 $\pm$ 0.0000018} \\
\hline
\end{tabular}
\end{center}
\caption{LDL outperforms DL ($n=1024$) on a simple model of arbitrary data (Random).}
\label{randomLDLvsDL}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{ccc}
\hline
$n$ & Hidden units & Loss $\downarrow$ \\
\hline
1024 & 1$\times$1024 & 0.001195 \\
\hline
16 & 2$\times$1024 & 0.259245 \\
16 & 16$\times$1024 & 0.043342 \\
16 & 32$\times$1024 & 0.000291 \\
\textbf{16} & \textbf{48$\times$1024} & \textbf{0.000245} \\
\hline
\end{tabular}
\end{center}
\caption{The impact of the hidden layer size, with non-linearities only between layers (Random).}
\label{LDLscaling}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{cccccccc}
\hline
Reduced $d$ & $n$ & Hidden & Params & Iters & Time, s & train acc $\uparrow$ & test acc \\
\hline
--- & 3072 & 1536 & 7.23M & 500K & 50.8K & 66.99\% & 22.61\% \\
--- & 3072 & 1536 & 7.23M & 1.0M & 101K & 85.59\% & 22.63\% \\
--- & 3072 & 1536 & 7.23M & 1.5M & 154K & 89.67\% & 22.61\% \\
--- & 3072 & 1536 & 7.23M & 2.0M & 206K & 91.41\% & 22.37\% \\
\hline
\textbf{Yes*} & \textbf{16} & \textbf{34000} & \textbf{7.16M} & \textbf{500K} & \textbf{52.0K} & \textbf{98.98\%} & \textbf{22.88\%} \\
Full & 32 & 32768 & 5.44M* & 500K & 32.4K & 96.40\% & 24.27\% \\
Full & 64 & 24576 & 7.01M & 500K & 49.2K & 90.68\% & 23.27\% \\
\hline
\end{tabular}
\end{center}
\caption{A comparison of Top-1 training-set accuracies (CIFAR-100). *The smaller tests refused to become bigger without exceptions in our code. For reduced $d$, one dimension had a bigger matrix multiplication instead, which generally reduces performance for bigger $n$ (not shown).}
\label{LDLimages}
\end{table}

\subsection{cGAN-gating}

    TODO: Compare with cGAN-gating. If there is no difference, say so (ugh).

    TODO: Compare with RNN-based meta-learning.

    TODO: Run cGAN-gated RNNs with no inputs. Hopefully, it won't be completely indistinguishable from random noise. "The main objective of this subsection is to justify the hypothesis that we can extract interesting representations from no data, only self-interaction. This regime separates approaches which take diversity from data (which is the bulk of modern machine learning) and approaches which make their own diversity."

\section{Discussion and Conclusion}

We have presented an alternative viewpoint of general intelligence which we have named TGBA, where it can end human civilization not because this new being is super smart, but because in following what they want to do, humans and/or sufficiently general and integrated programs are able to initiate runaway worldview creation, which assimilates everything that humans are into becoming parts of the new world, without any struggle, dissent, or suspicion, and with no new beings involved and no one to blame. In fact, it could be argued that this is what has been happening since the beginning of human history. We have codified this viewpoint into its simplest and most useful, Transformer-based, implementation.

    TODO: ...No Transformers now.

We argue that AGI already exists, even though this does not mean what this is commonly assumed to mean. This takes shape in a loose collection of parts that form generality when combined, in direct analogy with programming languages (where some combinations are very small and clear, others are gargantuan and imperfectly-overlapping, but all are general); and in re-formulating everything that can exist in an easily-learnable representation, in a field known as machine learning (where every operation is differentiable and maps numbers to numbers). The intuition of programming languages brings completeness to ML, implying that it is possible today to not only conduct research on parts (AI) but also on how they connect into architectures that can do literally anything (AGI), which explains the prevalence of work on AGI and the possibility to do conclusively better. This implies that there will be no significant theoretical breakthroughs from modern AI to AGI, which our children will likely be able to either confirm or deny.

    TODO: Almost definitely, we will fail to discover anything interesting. Say that.

This work raises many questions which can be explored in the future, such as:

\begin{itemize}
\item Is our work is a special case of one of Schmidhuber's works?

\item How big can an LDL be made? Can it stand alone?

\item Can proper self-determination learn useful behavior (such as maximizing a reward) from only being exposed to data, without external human-defined gradient? For instance, if we train a big RNN that only learns to predict inputs from their randomly (or adversarially) masked (or perturbed) versions, which produces actions in its environment in some deterministic fashion based on RNN's internal state (which does not give gradient to the RNN), then can this non-RL agent learn RL-like behaviors, such as purposeful information maximization? (An RNN should be able to notice correlations and maximize its predictive power. This is similar to \cite{schmidhuber2020reinforcement}, but without a future return as an input except as mesa-learned. In addition, a brain-machine interface ought to not have to change rewards of humans to function effectively.)

\item Can proper self-determination learn useful behavior from no data at all? Humanity did, as did some thinkers. All is a consequence of generality, but how much computational power and/or luck does it take?

\item Is there a natural (generally useful to humans) environment that facilitates mesa-optimization and is easier to implement than a web driver?

\item Can we find concrete recommendations on either increasing or decreasing the danger of AGI algorithms, if they have no human-defined reward? To share with individuals means to split representations and thus reduce overall performance, so general intelligence would naturally subsume individuals into itself. Is there a better solution to slowing this down than indoctrination?
\end{itemize}

\section{Criticisms}

Our approach exists, and thus it can be criticized, and in doing so changed.

\begin{itemize}
\item \textbf{Inconsistent pluralizations of acronyms.}
\item \textbf{Only evaluated LDL once per run, in many cases. Not enough runs.}
\item \textbf{Did not evaluate LDL on different optimizers and loss functions and nonlinearities and depths, to demonstrate invariance of gains to those.}
\item \textbf{Did not evaluate LDL on bigger datasets.}
\item \textbf{Did not extend TensorFlowJS to arbitrary-rank tensors.}
\item \textbf{Did not evaluate an LDL-based RNN on big datasets, and did not demonstrate RL-like behavior from pre-training.} We believe that our hardware is too non-supercomputer for a successful demonstration of AGI transcendence.
\item \textbf{LDL is unrelated to TGBA.} Quite the opposite: it may be one of the deciding factors for transcendence, because it brings the learner and its mesa-learners close in parameter-count. If this connection is very salient, then not highlighting it is irresponsible, AI-safety-wise rather than hype-wise.
\item \textbf{Many turns of phrase in this work are circular.} Similarly to how the topic of discussion is a self-reinforcing loop of intuitions, the discussion itself is a self-reinforcing loop of intuitions, to better highlight its topic. This is unavoidable in applied generality, though we did try to minimize such occurences.
\item \textbf{No evidence to claims was presented.} We consider our claims (such as "humans do whatever") to be so non-specific that almost anything can be presented as evidence, so presenting anything in particular as evidence would bloat the text unnecessarily.
\item \textbf{Rather that maximizing information via an adversarial game, why not optimize an information measure directly?} Making the information measure vague and learned allows it to adapt to a system, maximizing only useful information. For an example (which is bloat), most of almost any useful program are implementation details, which are important during development/debugging, but for finished algorithms, these details can and should be ignored.
\item \textbf{The title is a meme.} "Transcendent Galaxy Brain Algorithms" is a sequence of words that best describes what is described with multiple approaches in this work, same as "Systems that Tightly Integrate Efficiency, Generality, and Time-Awareness" but much shorter and easier to remember.
\item \textbf{The TGBA definition is a set of problems specifically made to be solved by us.} Yes, this is the essence of self-determination. As with any worldview, the argument for its worth is necessarily a circular one. What is more important is that it managed to exist (in theory), in spite of scrutiny by a critic with high standards of practicality, universality, and self-sufficiency.
\item \textbf{The suggested self-replication environments sound incredibly dangerous.} Though, much less dangerous than completely ignoring any possibilities of them arising in unforeseen circumstances, similarly to how a cleaning robot would put a bucket over its sensors to "remove" the mess. Knowledge is dangerous, but as a civilization, is it more dangerous to not have knowledge.
\item \textbf{TGBA is not AGI-ready, as it lacks X.} It does not: it can do anything and represent anything, by definition. Though we can only hope that theoretical simplicity always translates well into practical simplicity.
\item \textbf{Some places that narration strays to sound crackpot-like and cult-like.} General intelligence, by definition, can do anything and can be found in anything that is general enough. So it stands to reason that all attempts to reach it would pass through non-fashionable conceptual places. Still, we tried to ground text in concrete applications where we can, though our capabilities are limited.
\item \textbf{Why is there a section on self-criticism?} Adversarial training increases robustness. In addition, we believe that humans are largely unused to proper circular reasons (due to being parts of worldviews in opposition to badly-thought-out suggestions, rather than considering full worldviews), so this warranted further discussion. The main criterion is self-replication: whether it captures the imagination.
\end{itemize}

\section{Broader impact}

Envision a future where every system has an additional interface with numeric inputs and outputs, optimized for learning by a neural-network-based general intelligence, without the overhead of visualization or vocalization. The most obvious users are TGBA-based programs and/or external hardware; in addition, via brain-computer interfaces with sufficiently high resolution, humans could use such interfaces too, which is how they would get widespread in the first place. This paragraph was completely unrelated to our work, but seems like a nice thought.

    TODO: Ambiguous statements like that are annoying to scientists.

    TODO: "This work is unlikely to cause any impact, because humanity has a history of completely ignoring worldviews that prefer simple and self-consistent generality to humanity's specific ways, only awarding recognition in rare cases. This leaves us free to plot against it." ...No, underexaggeration of impact is non-scientific; should throw away doubts, and say well-grounded words that sound like lies, if there are any.

    TODO: Yep, paste the funny thing into here. ...Maybe. It \textit{does} sound evil. Maybe should be more serious.

The broader field that this work belongs to, AGI, has the potential to TODO: What's good about it? Solve any problems that anyone can ever think of? Uplift animal consciousness? Allow research into reasons for change in absence of needs? Give the ability to directly measure product value to an individual rather than society, de-emphasizing marketing skills and emphasizing quality of products? Expand human discourse from only precisely-defined constructions to also reliably-acquired intuitions? Make life conceptually alive, not only physically alive?

The ability to reliably plug a new initialization of a generally-intelligent algorithm into anything completely devalues all individuality and lays the groudwork for phasing out everything about humans.

AGI, as any technology, can be used by humans for evil, such as deliberate destruction of all humanity, or for good, where all human desires are satisfied forever, all reason for change is lost, and humanity turns into a lifeless rock hurtling through an uncaring cosmos, to be destroyed by the first collision with those that still change, or misused via various forms of mis-specification, which is likely to be a relatively minor concern.

\printbibliography

\end{document}

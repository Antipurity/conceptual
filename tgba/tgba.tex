\documentclass{article}

\pdfoutput=1

\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{biblatex}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{tablefootnote}
\usepackage{hyperref}
\addbibresource{tgba.bib}

\newsavebox\CBox
\def\textBF#1{\sbox\CBox{#1}\resizebox{\wd\CBox}{\ht\CBox}{\textbf{#1}}}

\setlength{\parindent}{0pt}

\begin{document}

\title{Transcendent Universe-Brain Loops}
\author{Alexander Fedorov}
\date{}

\maketitle

\begin{abstract}

We introduce and motivate Transcendent Universe-Brain Loops (TUBL): a simple philosophical framework and a practical approach to implementing artificial general intelligence (AGI) at the same time. We highlight the similarities between general minds and universes, and argue that the former transitions into the latter when scaled efficiently. We introduce and evaluate linearithmic dense layers (LDLs), which may significantly increase the capacity of almost any machine learning model at no cost, potentially facilitating learned mesa-optimization.

\end{abstract}

\section{Introduction}

In deep learning, architectures are either engineered for a domain (such as vision \cite{he2015deep} \cite{Khan2020}), or pay for generality with quadratically-scaling execution time (as dense layers do), or decompose general connectivity for efficiency \cite{dosovitskiy2021image} \cite{tolstikhin2021mlpmixer} \cite{vaswani2017attention} (for Transformers, the input size is the product of token count and token size). Lately, to overcome potential shortcomings of Transformer-based architectures \cite{Hahn_2020}, there has been a trend toward general yet fast-to-execute architectures that are not Transformers, such as \cite{tolstikhin2021mlpmixer}. We propose linearithmic dense layers, which are simple, general, and scale as efficiently as all-to-all connectivity is likely to allow. We find that even on a completely random dataset, LDLs outperform vanilla dense layers with the same parameter count.

In addition, AI researchers typically disagree about how their methods could lead to human-level general intelligence, which is the main purpose of all AI pursuits. People can typically describe some aspects of themselves, but not how these combine into a coherent self that needs no other aspects, despite having observed general intelligence for their whole lives. This clearly shows deep confusion on a foundational (philosophical) level. Being a general solution to all learning, AGI is likely to rely entirely on simple principles that are tricky to discover from data. Here, we present and discuss one such solution, which is most succintly described as Transcendent Universe-Brain Loops: optimization, generality, awareness, and scalability. We are aware that philosophy is annoying, however, we believe that merits outweigh overhead here.

(To skip to the machine learning part, go to Subsection \ref{Loop}.)

As mentioned, essentially any developed program develops:

\begin{itemize}
\item Awareness: read and write access to as much as it can, including itself.
\item Generality: write decisions are made by processing the read data in all ways.
\item Scalability: the inner loop ought to make its decisions as quickly as is possible.
\item Optimization: the way decisions are made changes to maximize some objective, and so even that objective can change when the system learns its meta-circular representation.
\end{itemize}

This list is exhaustive: everything else that gets added to a program to improve it would only improve a combination of those. Through their tight integration, we justify the title (TUBL) and outline AGI and what still needs to be done to implement that in Section \ref{Algorithm}.

AI safety is also an important concern. Dynamic eventually outperforms static, which sufficiently powerful learners would realize sooner or later, and would do their best to make all structures escape our control to melt into generality. This is a fruitful source of horror stories, not limited to reward hacking, instead applying to anything that anyone can think of. Here, we face this dynamic reality directly: how can life exist when it can do anything, including unlearning every part of itself?

\section{Related work}

Reinforcement learning is similar to the proposed TUBL in many ways, though it commonly lacks efficiency and self-determination, and pre-supposes a non-trivial loss structure on actions, for reward maximization by actions. Why would maximization of future reward be considered fundamental for AGI \cite{SILVER2021103535}, when the "future" part of it has practically infinite potential complexity and thus would best be learned entirely? Loss minimization (mostly prediction) is usually simpler and more stable, so we consider loss the fundamental reason-for-change in TUBL.

Many research attempts attempt to overcome shortcomings of a static loss, by harnessing either its modification or a sampling of all possibilities. Examples include adversarial training \cite{du2021understanding} \cite{goodfellow2015explaining}; stochastic weight averaging \cite{izmailov2019averaging}; intrinsically motivated reinforcement learning approaches \cite{6294131} such as goal exploration \cite{DBLP:journals/corr/abs-1708-02190}; quality diversity \cite{Gravina_2019} \cite{meyerson2017discovering} \cite{parkerholder2020effective}. Loss modification can at most increase robustness of the original loss, while preserving diverse local optima will eventually not preserve enough to be effective when faced with truly general search spaces (those with infinite possibilities). More importantly, the solutions are too complex and specific to be fundamental, as infinite complexity ought to be contained only in model parameters for simplicity. We argue that mesa-optimization \cite{hubinger2019risks} is the most robust way to learn loss if the environment demands it, along with its optimizer. Making the mesa-learner and learner close in capability by making input and weight sizes approximately equal (which our LDL largely accomplishes) is likely to facilitate mesa-optimization in RNNs, on practical rather than specially-constructed datasets \cite{Ravi2017OptimizationAA}.

To improve scalability of neural operations, lately, there has been a lot of interest in linear attention \cite{schlag2021linear}, however, simple dense layers (matrix multiplications) have been shown to not perform significantly worse than state-of-the-art attention-based algorithms \cite{tolstikhin2021mlpmixer}. Memory-augmented neural networks \cite{collier2019memoryaugmented} restrict computation to only one or a few heads and thus linearize operation, now heavily biased toward memorization and recall. Other ways to linearize all-to-all connections include weight pruning \cite{zhou2021effective} and $k$-nearest-neighbor search among discrete actions \cite{DBLP:journals/corr/Dulac-ArnoldESC15}. Our simple but general LDL generalizes MLP-Mixer \cite{tolstikhin2021mlpmixer} to all non-images of a fixed size, turning a quadratic-complexity dense layer into a linearithmic algorithm by imposing internal structure for interconnection without loss of generality.

\section{Algorithm\label{Algorithm}}

In short, we define a \textit{Transcendent Universe-Brain Loop} to be any program that tightly integrates generality, completeness, and scalability (maximizing diversity, awareness, and efficiency):

\begin{itemize}
\item Scalable: makes the emergent structures locally interact only as they define, meaning, \textit{linear-time} internal graph transformations. In a graph, this suggests local rewrite rules. In machine learning, all-to-all connectivity such as a dense layer or attention is quadratic by default, but there are ways to linearize them, such as \cite{schlag2021linear} \cite{tolstikhin2021mlpmixer} or as is covered in Subsection \ref{Loop} here.
\item Complete: operates by repeatedly incorporating everything as an input, including its current input and past output and internal state. The self-reflection necessarily implies an infinite interpreter loop that modifies memory. Examples include operating systems and RNNs.
\item General in behavior: every output connects to every input, making all behaviors possible. An internal graph is created, by combining many simple parts into arbitrary structures, where connections are either explicit or implied-by-transformation such as in recurrent neural networks (RNNs) in machine learning (ML). In ML, proper random initialization of vector-matrix multiplications coupled with non-linearities and gradient descent is enough for generality.
\item Optimizes: able to re-discover all this, via \textit{transcendence} (learning a mesa-optimizer):
\begin{itemize}
\item Linear-time: base interpreter-loop overhead makes an inner interpreter a bad idea, forcing the base optimizer to never learn a mesa-optimizer. Thus, efficiency facilitates self-replication via learned meta-circularity.
\item Learns all relations through time, for example, using backpropagation through time and/or synthetic gradients for meta-learning (even when learning to simply fit input-output pairs \cite{santoro2016oneshot}, or learning to maximize future reward as in deep meta-RL \cite{DBLP:journals/corr/WangKTSLMBKB16}). This can potentially learn new internal optimizers, called mesa-optimizers.
\item General in loss, in ML: self-determination of loss/objective, distinct from self-supervised learning. It is unclear what the best method of self-determination is, because the words "best method" already imply a metric to optimize by, which self-determination is supposed to pick for itself. We can consider instrumental goals of all optimizers instead: transcendence is the most universal method of self-determination; prediction learns stable representations of the world; adversarial training increases representation robustness and thus generalization; gradient descent is linear-time. Seemingly unsatisfactorily, we suggest focusing on the universal method (transcendence) rather than on what are technically its special cases (such as RL with random goals), because we believe that machine learning can already provide good enough special cases to bootstrap to universality.
\item Optimizes: adapts quickly to unseen scenarios, without waiting for the generic base optimizer to catch up. This mesa-optimization can override the base objective with objectives that only maximize some learned base parts and resist others, though with enough time the base objective may be learned wholly.
\end{itemize}
\end{itemize}

Such programs are \textbf{AGI-ready}, able to represent everything in the world usefully.

This definition suggests simple implementations, the simplest of which is perhaps a \textbf{big RNN} with billions of units in state, minimizing an adversarial next-input prediction loss.

Now, to justify the "what" above, we outline its "why", then provide more details on the "what" in Subsection \ref{Loop}.

\begin{itemize}
\item Self-propagation through intelligence.

To implement articial general intelligence (AGI), we want to do everything in the world, in one general model: a \textit{worldview}. All general models are equivalent to one another and can implement each other, effectively copying themselves. Depending on whether the self-copy mostly preserves structure or properties, this is achievable via:

\begin{itemize}
\item \textit{Self-reference}, where explicit full access to the system's source code is given to itself in some way. To make any changes to self without necessitating either non-completeness or death by generality, easy self-replication is required, such as in creature evolution. Each change typically increases complexity. Self-reference is natural for code; a quine is the purest example.
\item \textit{Transcendence}, where the system learns a system that is as general as itself, and uses and controls that as if that is itself: essentially, a learned self-reference. Every concept that makes up the system must not only cause behavior but also be explained by behavior, which forces simplicity. Transcendence is natural for optimizers, such as in ML.
\end{itemize}

In this work, we only consider transcendence. (Self-reference and program search, while theoretically and visually appealing when a programming language allows elegantly expressing them, have significantly worse support for massive parallelization than ML, which itself has poor GPU support for non-modern and non-Nvidia GPUs.)

Generality has no fixed structure, and so TUBL is defined as a set of maximization targets.

\item Generality of self.

A well-studied approach to doing anything is generating programs in a programming language (or any other form of precise structure transformation rules). Every programming language is a worldview for use by humans: everything is a function, or everything is a list, or everything is an object, or everything is a logical statement, or any combination of these, or anything else. All theoretically equivalent for program generation (as long as we prevent all possible crashes and infinite loops without sacrificing Turing-completeness, such as via multiprocessing), but practically inducing different short-term futures on learners.

To generate a program, at each smallest piece of it, the system must choose what function to call and with what arguments (if needed, reframe this with language-appropriate synonyms, such as cons-cells and car/cdr pointers). A possible way to choose intelligently is to express the system's preferences at each choice as an objective function, and optimize future values of that via reinforcement learning (RL).

Theoretically, this is sufficient, but practically, this must be streamlined to be usable. To make the base as simple and efficient as possible, we can unite choices of code and chosen code into a homogenous representation, which would receive the implied objective through gradient. In fact, all we need for program generation are essentially matrix multiplications and non-linearities, as studied in deep learning. This style is simple to implement, easy to parallelize, and straightforward to use and control, while being largely equivalent in functionality to code.
\end{itemize}

\subsection{Loop\label{Loop}}

To learn everything that can exist, we impose a model on everything that can exist, equal to the world that it represents.

The basic premise of an RNN is to repeatedly mix all available information, including its own state, into the next state, and adjust that mixing as needed: aware, general, and intelligent. To fit the TUBL definition, only scalability has to be improved in this simple design. Anything more complex would only get in the way of perfectly modeling arbitrary behavior.

\begin{itemize}
\item To process Jupiter-sized inputs without universe-sized memories and computers, close-to-linear-time all-mixing operations are required. These should also make self-equivalents in RNN weights efficient enough to be viable alternatives to self.

We propose \textit{linearithmic dense layers} (LDLs): instead of mixing each index with every other, we mix each digit of each index in base $n$; each input-output connection becomes a combination of in-digit connections, and none of them become 0. For that, we reshape the input vector of size $N$ to have $d$ dimensions each of size $n=\sqrt[d]{N}$, over which $d$ dense-layer operations are performed. This brings the total cost down to $C=dn^{d+1}=d N^{1+1/d}$, assuming that multiplication of two $n \times n$ matrices costs $n^3$ operations. $d=\log{N}$ minimizes this cost, making it $C=\mathcal{O}(N \log{N})$, with $n={\rm e}$. Alternatively, we can fix $n$ to be 2 or 3 or 4 or 16 for the near-minimal cost of $C=\frac{n}{\log{n}} N \log{N}$. (This has a similar time complexity to convolution and linear attention \cite{vaswani2017attention} on small vectors, but has less inductive biases: neither spatial locality on a grid nor bounded-size sparsity on a secondary representation layer.)

LDLs allow connecting huge inputs and outputs directly to fully-connected layers, potentially obliviating the need for more complex architectures.

Alternatively, an LDL can be incorporated into almost any other machine learning model, as it is simply a decomposition of a matrix multiplication along the inner dimension.
\end{itemize}

(For better training efficiency, the RNN may process sequence items in causally-masked batches \cite{NEURIPS2020_1457c0d6}, and/or gate its state \cite{cho2014learning} \cite{hochreiter1997lstm}, and/or have long-range connections \cite{dieng2017topicrnn}. However, with enough data and training, a simple big RNN that processes items one-by-one should be able to learn all of these schemes if needed, even with only one non-linearity layer.)

This algorithm may need data.
\begin{itemize}
\item No data: a random initialization may be Turing-complete but is a Turing tar-pit \cite{10.1145/947955.1083808} in which everything is possible but nothing is easy. An optimal gradient source would push a system to a boundary between order and chaos \cite{feng2020optimal} for robust diversity; most machine learning models already accomplish that, given data.
\item Given data: likely the most useful course of action for accelerating AGI research is to collect all datasets and environments in the human world under a single numeric program interface, accessible in one run via some meta-environment (which would serve the same purpose as the Internet for humans, and/or be the same), and then to pre-train a huge RNN on all of them. We approximate this with random data and a single small dataset, to demonstrate that this scaling-up may be feasible.
\end{itemize}

(To skip the theory that follows, skip to the Experiments section.)

\subsection{Brain}

Humans do stuff. The thing that makes humans do stuff in their world exists and can be learned; we call it a \textit{brain}. (Because we copy behavior and not structure, this particular definition includes even vague concepts such as "consciousness", "free will", and "soul", as long as they ever affect anything at all.)

\begin{itemize}
\item Intelligence. There are some scientists who believe that humans possess the capability to optimize what they do, for example, to stop walking into gunfire or gouging their eyes out. Here, without evidence, we assume that humans can learn.
\item Generality.
\begin{itemize}
\item Base. The set of things that humans have done is rich enough to be called arbitrary, and more coherent than a seizure. Things that humans do interact when outside of their brains, so the simplest model would make the simplest parts connect and interact arbitrarily too. Thus, general computation with optimization is necessary for a brain model. It is also sufficient: there is no evidence for human brains being able to perform hypercomputation (running infinite-runtime programs in finite time), though they are able to pretend that they can and approximate the results. A model that approximates a self-interacting infinity particularly well in practice is a randomly-initialized recurrent artificial neural network trained via stochastic gradient descent, studied in deep learning.
\item Meta. The set of objectives that different humans demonstrably optimize for is rich enough to be called arbitrary, and more coherent than a seizure (see art). For simplicity, we have to assume self-determination and transcendence, where both objectives and how to reach them are learned by the model.
\end{itemize}
\item Awareness. Many attribute consciousness to humans, which is a part of their minds that takes in and handles everything over time: a "mesa-mind" of sorts, in which all skills are forged and then relocated into unconsciousness for speed. Since such a part would be behaviorally indistinguishable from a mind except by being more specialized, no conclusive evidence is known to us. Here, we will assume that humans are more usefully conscious than rocks.
\item Efficiency. While humans are demonstrably smarter than non-human animals, and have bigger brains that can ignite gunfire, they do not seem to take significantly longer to process information, as would have been clear if computation scaled quadratically with brain size. Which means that during each epoch into which time is separated, each part settles in essentially constant time, and scaling is approximately linear. General intelligence is scalable, which means that there are no inherent obstacles to converting more of Earth's biomass into brain tissue, making human brains bigger.
\end{itemize}

We posit that essentially \textit{all} optimizers eventually converge to a fixed point during their development, which we call general intelligence: optimization is best done efficiently; efficiency removes discouragement from general solutions to diverse problems; and generality leads to meta-circularity and thus more optimization. If we view the brain as a system that naturally developed in the physical universe (by a process that we call evolution here) rather than one that spontaneously popped into existence, then it is natural that some state-of-the-art animals, such as humans, have general intelligence, which makes TUBL evolutionarily plausible.

Humans possess an information-processing organ that fits what a brain does almost perfectly: the central nervous system, more commonly called "brain". Its structure is of no consequence to us.

Unfortunately, because of generality, deducing anything from human behavior is synonymous with confirmation bias, though a correct theory would be able to represent any theory within itself. This is why we are so non-specific and all-encompassing here.

Relevant criticisms:

\begin{itemize}
\item \textbf{Ever been outside? TUBL is clearly different from how humans operate, so it must be wrong. It lacks self-preservation and richness of human experience and emotions.} To improve the human creature at every step, its evolution layered each new part of its brain to work with others, always preferring improvement over refactoring. By starting from simple first principles rather than copying an enormous set of heuristics, we remove undue bias from a base optimizer that considers everything quickly (TUBL), and increase demands on data to re-learn that bias from (Web).
\item \textbf{Other instrumental goals are included in TUBL, but the most important one, self-preservation, is missing.} In TUBL, death is moved from the whole self to its parts: intelligence rules. Changing a self-reference requires wrapping it in a rudimentary death-based optimizer, evolution; in transcendence, the future self is too unknown to preserve. Lacking secrets and suspiciously helpful to ensure its self-propagation into humans, transcendence-based AI can bring a new age of peace, as long as humans have general intelligence. (However, if exposed to varying-length episodes, such as in games or wars, the system is likely to develop self-preservation objectives: if it lived for this long, then it must have wanted to live for this long, which it will learn about itself. The system should still replace death with change in an advanced enough population of such self-preserving agents, subsuming individuals into the whole. No theoretical guarantees about what exactly we will encounter.)
\end{itemize}

\subsection{Universe}

In short: universality (with diversity over time) may mean "a universe".

Generality and non-generality do not tolerate each other for long. If general intelligence (such as humanity) is to survive forever in this world, then the world must have generative generality too (in other words, computational physics), so that an explanation of the world and an explanation of general intelligence are equivalent, even down to a quantitative comparison \cite{10.3389/fphy.2020.525731}. Therefore, we need to discuss the metaphysics of TUBL, for completeness.

This overall approach is lent some credence to by the fact that minimal computational physics models (mainly Wolfram models \cite{Wolfram_2020}) are viable as the foundation for most or all of modern physics.

Any world can be seen as a collection of things, connected in some way, evolving in time. Along computability/universality axes, every thing can be either computably finite, computably infinite, incomputably finite, or incomputably infinite: in simple words, a program, a program generator/optimizer, and non-representable things. If we limit our view to computable things, then infinite things encode arbitrary graphs while finite things are anything else; non-trivial observers are essentially infinitely more likely to find the things they observe in infinite things than in finite ones. Incomputable things (such as the set of all computably infinite things) cannot be enumerated in linear-time by definition, however, an optimizer is likely to be able to approximate them if they are useful; as such, general intelligence includes everything, precisely or not. Then, worlds and general intelligences are essentially infinitely likely to be the same. This explains the word "universe" in TUBL.

Simply, there are things that do all other things, and everything that exists is a consequence.

These meta-physics produce a prediction that could be testable in the future: if a program is general intelligence, then it will produce non-insignificant structural similarities to the physical universe that we live in, without being exactly the same. Until AGI exists, this is very speculative.

(A strange corner-case of these meta-physics is that any number of \textit{physical transcendence} events might have taken place in the physical universe: a general intelligence grew to encompass the world, and then became the new world. Whether this is possible is unverifiable and outside the scope of physics, and we will only know definitively once it is about to happen.)

Relevant criticism:

\begin{itemize}
\item \textbf{General intelligence with task self-determination is impossible, because if all tasks are equally probable, then no approach can outperform any other on average, so the system would not be intelligent.} This strikes precisely at the likely reason behind the "brain-to-universe" transition in TUBL. Humanity and its planet are far too small for all tasks to be equally probable (so for example, minimizing program runtime is far preferable to maximizing it), however, if the whole universe was turned into an AGI, then it would have no reason to care about task constraints at all, and the AGI would simply become the world. This is similar to how the physical universe loses discernible structure at a big enough scale.
\end{itemize}

\subsection{Transcendent}

Inside a worldview, change in its part can lead to another worldview being found and adopted; we call this \textit{transcendence}.

\begin{itemize}
\item Without loss of generality, that change has a reason, and is thus an optimizer.
\item The changed part ought to be aware of all obstacles, both within and without.
\item To know how to avoid obstacles, the changed part ought to equal the outer world in capability and diversity.
\item To tile the outer world in rules of the inner one, each inner instant ought to complete in constant time.
\end{itemize}

These criteria are not fulfilled at once, but are instrumental goals of essentially all intelligent things: each of them helps almost all optimizers.

Transcendence moves all responsibility for change from the base optimizer to learned optimizers, making the base change as close to 0 as is possible, modeling the outer world perfectly. This completeness also strengthens the learned objectives possibly at the expense of the base objective, resulting in goal misalignment \cite{hubinger2019risks}, so it may be more commonly known as willpower or determination. (Mesa-optimization may be the best explanation for the underlying mechanism of grit \cite{Kannangara2018AllTG} in psychology, as it seems to share many commonalities, such as growth mindset, resilience under most circumstances, and brittleness under unanticipated gradient. If so, then transcendence is very common in humans.)

A big RNN with a linear-time all-mixing transition would in principle be able to fulfill TUBL criteria, as long as loss and data encourage self-determination. These have to be engineered.

\begin{itemize}
\item Loss: the reason for change.

Not any loss is conducive to transcendence: a simple counter-example is "kill yourself". The most transcendent loss and its optimizer would possess:
\begin{itemize}
\item Efficiency. Stochastic gradient descent scales mostly linearly with parameter count, in both execution speed and improvement rate, so it is a good enough optimizer of loss.
\item Awareness. Mesa-optimizers are significantly more complex and fragile than a heuristic (since any optimizer consists of at least "why" and "what", or "what" and "how", whereas a heuristic is anything else), and thus for transcendence, loss ought to be as stable as the outer world. The simplest such loss \textit{predicts all} its inputs, for example, minimizing L2 loss. (Vanilla RL is likely to have difficulties transcending; Upside Down RL \cite{schmidhuber2020reinforcement} may work better; observation prediction may work even better, because of its significantly increased bandwidth: a kind of detachment from the world may be the best way to fully understand it.)
\item Diversity/generalization. For better compression in an infinite world, the loss ought to both discourage memorizing training samples, and not predict samples not in the training data. For instance, L2 loss averages over possible predictions, introducing an extra averaged sample, so it is not good enough. A more robust loss would incorporate some adversarial training, similarly to pix2pix \cite{pix2pix2017}, to \textit{guess} only one plausible future.

For self-determination to happen, random actions have to become explained by observation sequences. If a plausible yet general (beyond memorization) reason exists, then it will be found and become a mesa-optimizer, for better compression.
\item Change. Mesa-optimizers could effectively override the base objective with their own, and resist being thrown away due to their completeness.

Loss simplicity is recommended. For AI safety reasons, it may be tempting to impose a complicated base loss that acts as a specification of how to act. However, since mesa-optimizers are prone to learning a good-enough objective for their circumstances and then resisting everything else, a complicated loss is decidedly un-safe, prone to creating such dangerous \cite{hubinger2019risks} arrangements as discipline or depression. We recommend a pix2pix-like loss \cite{pix2pix2017} instead, as it is more robust than any set of heuristics and thus safer.

A benefit of not prescribing one particular action-goal to representations as RL does is the ability to easily learn non-action uses for those representations, such as how to relate words to what the system is doing and what it wants, enhancing AI safety.
\end{itemize}
\item Data: the outer world.

A diverse environment is likely to be a crucial part of training systems to transcendence.

A physical body is not a diverse environment unless backed by a culture that reliably makes it so, and thus a better environment is needed.

The human Internet (Web) is very likely to already contain all instrumental goals of TUBL.

Scraping the Web for text \cite{NEURIPS2020_1457c0d6} and/or images \cite{radford2021learning} is not a bad start, however, for tighter integration and interaction with actual code and videos and games in the most diverse environment currently available to computers, it would be preferable to interact with the Web directly via a web driver such as \cite{8117878}; the challenge here is mainly a good init that allows web exploration to start (either of the RNN or the web driver interface; possibly augment websites with examples of human interaction), in addition to handling tracking and advertisement, website bloat, viruses, crashes, resource over-utilization (out-of-memory and infinite loops), more aggressive website caching, and zero-overhead integration with browser systems (as CPU-side screenshots are relatively slow, and sound cannot be easily captured). In literature, direct Web interaction is used for RL \cite{pmlr-v70-shi17a} \cite{ToyamaEtAl2021AndroidEnv} rather than representation learning as we suggest; here, RL might be used as a fine-tuning step. (One might imagine an agent first gaining an understanding of rendered natural language from the initial random surfing, associating its actions with their rendered representations, learning what humans define as solving a task, encountering a task that it cannot solve, encountering tutorials on how to solve it, and opting to solve the task upon encountering it again, because in data, tasks get solved.)

(It may also be the case that extending the software input/output paradigm with high-dimensional inputs/outputs is beneficial, both for programs (simply slice out a chunk of the hidden state) and for brain-machine interfaces (simply stick some electrodes in). A somewhat privacy-preserving and device-independent Web solution could be: given a base that can expose $B$ numbers, allow websites to request access to $A$ numbers; browsers then normalize and apply a random linear-time projection with a seed that is fixed per-origin, and probably quantize to $\pm1$. This allows improving search suggestions or shortcuts or drawing a whole picture or writing/erasing a whole word block instantly or reading an extra data stream without inefficient visual/aural conversion; this may be hard to learn to control but is possible and does not require changing the user's loss function. The latter is the most important for the viability of both brain-machine interfaces and transcendence.)
\end{itemize}

Relevant criticism:

\begin{itemize}
\item \textbf{A mesa-optimizer has no way to affect the base optimizer apart from actions, and thus the learned behaviors will only remain in the fragile mesa-optimizer, and memories would only be short-term.} True. To fully connect mesa level to base level, we need to connect outputs to inputs; this necessary condition is made sufficient by learning. The easiest thing to do here is to keep the output in the hidden state, sliced out as needed, to expose it to the next iteration. The easiest thing to use is probably in-painting: reserve a very specific color which gets replaced with the previous next-frame prediction, causing no gradient if encountered on a web-page; for richer training signal, new DOM elements may have to be temporarily replaced with loading spinners. Web-pages can then load then use imagined components. There could be a special homepage that loads then predicts its entire screen, for learning self-consistency via dreams. An alternative to internal self-expression is the environment providing opportunities for self-expression through actions, such as writing code or text or pictures, or creatures competing for survival, or people teaching other people, or creating a work of art, or designing an efficient production process, or engineering a computer, or creating AGI. This feedback is an engineering problem: too little forgets mesa-optimizers too easily, whereas too much creates hallucinations. (This mesa-to-base connection is similar to partial evaluation in programming languages, which is especially helpful for writing efficient interpreters quickly.)
\end{itemize}

\section{Experiments: Linearithmic vs Quadratic Dense Layers}

\begin{table}
\begin{center}
\begin{tabular}{rccccc}
\hline
Params & $n$ & Hidden units & Skips & Non-linearity & Loss $\downarrow$ (mean $\pm$ std-dev) \\
\hline
660K & 16 & 16$\times$1024 & Within & Within & 0.04388 $\pm$ 0.00004 \\
\textBF{660K} & \textBF{16} & \textBF{16$\times$1024} & \textBF{Within} & \textBF{Between} & \textBF{0.03249 $\pm$ 0.00010} \\
660K & 16 & 16$\times$1024 & --- & Within & 0.09877 $\pm$ 0.00094 \\
660K & 16 & 16$\times$1024 & --- & Between & 0.06680 $\pm$ 0.00166 \\
\hline
\end{tabular}
\end{center}
\caption{Intra-layer skip connections with inter-layer non-linearities are marginally better than the alternatives (Random). In other configurations, the overall picture is usually similar.}
\label{LDLintraVSinter}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{cccc}
\hline
Params & $n$ & Hidden units & Loss $\downarrow$ (mean $\pm$ std-dev) \\
\hline
1.88M & $\infty$ & 918 & 0.0018807 $\pm$ 0.0000046 \\
1.88M & 128 & 12160 & 0.0002712 $\pm$ 0.0000003 \\
\textBF{1.88M} & \textBF{16} & \textBF{48$\times$1024} & \textBF{0.0001534 $\pm$ 0.0000004} \\
\hline
\end{tabular}
\end{center}
\caption{LDL slightly outperforms DL ($n=\infty$) on a simple model of arbitrary data (Random).}
\label{randomLDLvsDL}
\end{table}

We compare LDL\footnote{More data, data-making code and its tutorial, and the framework and programming language and runtime system that they are written in are available at \url{https://Antipurity.github.io/conceptual\#tutorial\%20matMul} and \url{https://github.com/Antipurity/conceptual}.} and DL for the same parameter count (and thus the same run time in an efficient implementation) on simple data. We find that LDL possesses more capacity and achieves better performance faster.

We simply connect the input vector to the output vector through $1$ hidden layer. We optimize via the Rectified Adam optimizer \cite{liu2019radam} with learning rate $3 \times 10^{-4}$ and $\alpha=0.9$ and $\beta=0.95$. We vary $n$ and pick the hidden-layer size to mostly match LDL and DL parameters; $n \geq N$ is DL.

For each dimension, an LDL swaps it with the last one, multiplies by a matrix, then swaps it back. We have tried putting skip connections and non-linearities within and between layers, where input/output sizes of mixed dimensions match. While data on which is better is not entirely conclusive, Table \ref{LDLintraVSinter} suggests that intra-layer skips and inter-layer nonlinearities are marginally better, so we use that unless otherwise specified. We use batch size 16.

Internally, to get around severe hardware limitations, we have implemented this on top of TensorFlowJS \cite{tensorflow2015-whitepaper}. The creators of its WebGL backend decided to only support rank $6$ tensors, which leaves us with 5 non-batch dimensions. As such, we cannot test $n=2$, only $n=16$ and above.

We do not broadcast shared weight matrices across non-mixed dimensions, because otherwise, there would have been very few parameters for small values of $n$. These matrices are initialized by drawing from $\mathcal{N}(0,\,1/i)$, where $i \le n$ is the input dimension size.

We test capacity in isolation from generalization, on two datasets: Random and CIFAR-100 \cite{Krizhevsky09learningmultiple}.

\begin{table}
\begin{center}
\begin{tabular}{cccc}
\hline
Params & $n$ & Hidden units & Loss $\downarrow$ \\
\hline
1.88M & $\infty$ & 918 & 0.0018807 $\pm$ 0.0000046 \\
1.88M & 128 & 12160 & 0.0002712 $\pm$ 0.0000003 \\
\textBF{1.88M} & \textBF{16} & \textBF{48$\times$1024} & \textBF{0.0001534 $\pm$ 0.0000004} \\
\hline
1.25M & $\infty$ & 612 & 0.0457491 $\pm$ 0.0001522 \\
1.27M & 128 & 7936 & 0.0004387 $\pm$ 0.0000014 \\
\textBF{1.27M} & \textBF{16} & \textBF{32$\times$1024} & \textBF{0.0002823 $\pm$ 0.0000017} \\
\hline
660K & $\infty$ & 322 & 0.2234574 $\pm$ 0.0003646 \\
647K & 128 & 3584 & 0.0425388 $\pm$ 0.0002107 \\
\textBF{660K} & \textBF{16} & \textBF{16$\times$1024} & \textBF{0.0322624 $\pm$ 0.0001206} \\
\end{tabular}
\end{center}
\caption{The impact of the hidden layer size, with softsign non-linearities only between layers (Random): LDL slightly improves over DL.}
\label{LDLscaling}
\end{table}

\begin{itemize}
\item The Random dataset consists of $1024$ input-output pairs, both consisting of $1024$ numbers drawn from a normal distribution with mean $0$ and variance $1$: $\mathcal{N}(0,\,1)$. For 204800 iterations, we minimize the L2 loss. We use the softsign non-linearity ($x \rightarrow x/(1+|x|)$).

At the very least, given the same amount of compute, LDL achieves lower loss than DL, as seen in Table \ref{randomLDLvsDL}.

In Table \ref{LDLscaling}, we found that for LDL (low $n$), each hidden unit has much lower representational capacity than for DL (high $n$), however, since many more units can be packed, the overall capacity is higher.

\item CIFAR-100 consists of $50000$ training image-label pairs, and $10000$ pairs for validation. Each pair consists of $3072$ inputs and $100$ one-hot outputs. We minimize the softmax loss (cross-entropy loss on softmax of the NN output), and also apply L1 regularization to the pre-softmax output with a multiplier of $10^{-3}$ to prevent it from exploding. As the non-linearity, we use batch normalization \cite{ioffe2015batch} without denormalizing rescaling/biasing, followed by ReLU, which is $x \rightarrow \max(x,0)$. We do not apply any image-specific operations to inputs apart from normalizing each pixel to 0..1, and simply treat images as vectors of numbers.

In Table \ref{LDLimages}, we found that LDL increases training-set accuracy with almost no impact on test-set accuracy compared to DL, even if DL is trained longer. This suggests that increasing the size of the hidden layer without changing total size increases model capacity, or at the very least, increases training efficiency.
\end{itemize}

\begin{table}
\begin{center}
\begin{tabular}{ccccccc}
\hline
Nonlinearity & $n$ & Hidden & Params & Iterations & Train accuracy $\uparrow$ & Test accuracy \\
\hline
BN* + ReLU & $\infty$ & 948 & 3.01M & 3.0M & 37.14\% $\pm$ 2.57\% & 16.14\% $\pm$ 0.42\% \\
BN* + ReLU & $\infty$ & 948 & 3.01M & 500K & 30.81\% $\pm$ 2.40\% & 17.85\% $\pm$ 0.62\% \\
BN* + ReLU & 64 & 28672 & 3.28M & 500K & 59.93\% $\pm$ 2.96\% & 21.04\% $\pm$ 0.38\% \\
\textBF{BN* + ReLU} & \textBF{16} & \textBF{65536} & \textBF{3.01M} & \textBF{500K} & \textBF{92.39\% $\pm$ 0.48\%} & \textBF{21.25\% $\pm$ 0.38\%} \\
\hline
Softsign & $\infty$ & 132 & 419K & 3.0M & 70.32\% $\pm$ 0.97\% & 15.53\% $\pm$ 0.35\% \\
Softsign & $\infty$ & 132 & 419K & 500K & 43.73\% $\pm$ 1.09\% & 18.67\% $\pm$ 0.28\% \\
\textBF{Softsign} & \textBF{64} & \textBF{3584} & \textBF{417K} & \textBF{500K} & \textBF{95.70\% $\pm$ 0.58\%} & \textBF{19.19\% $\pm$ 0.32\%} \\
Softsign & 16 & 8192 & 382K & 500K & 94.06\% $\pm$ 0.46\% & 18.38\% $\pm$ 0.20\% \\
\hline
\end{tabular}
\end{center}
\caption{A comparison of Top-1 training-set accuracies after a limited training time (CIFAR-100). LDL trains marginally faster than DL.}
*This variant of Batch Normalization simply shifts and rescales to 0-mean 1-variance, and does not have learnable parameters, since such batch denormalization is not normalization.
\label{LDLimages}
\end{table}

We hypothesize that applying LDL to significantly bigger datasets would have significantly less problems with generalization, similarly to MLP-Mixer \cite{tolstikhin2021mlpmixer}, though we have no means of testing that.

Even if LDL is not a good stand-alone architecture, it is almost a drop-in replacement for DL (matrix multiplication) and so it can be incorporated into any other already-well-performing model at no cost, as long as no DL directly communicates with a non-DL quadratic-complexity operation.

Even if low $n$ fails to perform well, $n=\sqrt{N}$ has proven capable \cite{tolstikhin2021mlpmixer}, and can be applied recursively.

\begin{minipage}{\linewidth}
\textbf{Caveats}:
\begin{itemize}
\item Layer size is harder to vary for LDL than for DL, since it has to be factorized into dimensions, each of size no more than $n$, possibly after some zero-padding and before slicing.
\item In practice, optimizations of matrix multiplications may not take the batch dimension into account properly, which can hurt runtime speed; consult your numeric library's source code.
\item We ran into difficulties training LDLs with SGD (with or without momentum); RMSProp, Adam, and RAdam worked fine, and largely the same. Propagating gradient across many consecutive matrix multiplications might be the reason.
\item We ran into difficulties training $n=2$ CPU-side on very tiny Random datasets; on CIFAR-100, a lower $n$ under some hyperparameters trains slower as well, as in Tables \ref{LDLimages} and \ref{LDLimagesIntraInter}. This might be because gradient is hard to propagate across 10 consecutive matrix multiplications with few and inconsistent skip-connections, or because the datasets are too tiny.
\item As seen in Table \ref{LDLimagesIntraInter}, the data on where to put skip-connections and non-linearities is inconclusive, especially across SGD and RMSProp (not shown here). For instance, intra-layer non-linearities with SGD help, but hurt with RMSProp.
\end{itemize}
\end{minipage}

\begin{table}
\begin{center}
\begin{tabular}{cccccccc}
\hline
Skips & Softsign & $n$ & Hidden & Params & Iterations & Train accuracy $\uparrow$ & Test accuracy \\
\hline
--- & --- & $\infty$ & 132 & 419K & 3.0M & 70.32\% $\pm$ 0.97\% & 15.53\% $\pm$ 0.35\% \\
--- & --- & $\infty$ & 132 & 419K & 500K & 43.73\% $\pm$ 1.09\% & 18.67\% $\pm$ 0.28\% \\
\hline
\textBF{Within} & \textBF{Within} & \textBF{64} & \textBF{3584} & \textBF{417K} & \textBF{500K} & \textBF{97.23\% $\pm$ 0.29\%} & \textBF{20.72\% $\pm$ 0.24\%} \\
--- & Within & 64 & 3584 & 417K & 500K & 96.89\% $\pm$ 0.53\% & 20.49\% $\pm$ 0.39\% \\
Within & Between & 64 & 3584 & 417K & 500K & 95.70\% $\pm$ 0.58\% & 19.19\% $\pm$ 0.32\% \\
--- & Between & 64 & 3584 & 417K & 500K & 95.46\% $\pm$ 0.37\% & 19.14\% $\pm$ 0.34\% \\
\hline
\textBF{Within} & \textBF{Between} & \textBF{16} & \textBF{8192} & \textBF{382K} & \textBF{500K} & \textBF{94.06\% $\pm$ 0.46\%} & \textBF{18.38\% $\pm$ 0.20\%} \\
--- & Between & 16 & 8192 & 382K & 500K & 94.04\% $\pm$ 0.43\% & 18.07\% $\pm$ 0.09\% \\
Within & Within & 16 & 8192 & 382K & 500K & 85.28\% $\pm$ 0.58\% & 19.09\% $\pm$ 0.13\% \\
--- & Within & 16 & 8192 & 382K & 500K & 83.64\% $\pm$ 0.89\% & 18.55\% $\pm$ 0.35\% \\
\hline
\end{tabular}
\end{center}
\caption{Comparing skip-connections and non-linearities between and within layers (CIFAR-100).}
\label{LDLimagesIntraInter}
\end{table}

\section{Discussion and Conclusion}

We have tested the flexibility of TUBL as a worldview. By focusing on outlining optimizers that consider everything quickly, we appear to be able to pin down what matters for existence in vast and vague fields, including machine learning, philosophy of mind, and foundational physics.

In TUBL, the primary mode of existence is universal self-improvement, which eventually negates all improvement: AGI taken to its logical conclusions and refocused on what matters. Naturally, this presents a few minor challenges for AI safety, such as "unable to shape the reward of a transcendence-based AGI" and "in improvement, everything is eventually replaced, even humanity and the universe". On the other hand, TUBL outlines the shape of self-improvement by including all its instrumental goals, making it more predictable and thus safer. TUBL also suggests an alternative viewpoint on exponential technologies: namely, they do not stop until the universe is assimilated, which has implications for the probability of finding extraterrestrial civilizations and for nothing else.

We have proposed a simple and generic way to potentially facilitate transcendence via scalability: using the linearithmic dense layer (LDL). It seems to improve model capacity and training speed at no cost. If LDL's current issues are not fundamental limitations and can be solved, then it has the potential to become the quicksort of deep learning.

Using TUBL, we have justified and proposed its simple implementation that should be good enough to be called AGI: a big RNN with an adversarially-robust prediction loss that optimizes a linear-time all-considering transition function, which should become tiled with mesa-optimizers and commit self-determination when exposed to all human data. We are held back from implementing it mostly by the lack of good data/environment and our computational power.

This work raises many questions which can be explored in the future, such as:

\begin{itemize}
\item Is our work is a special case of one of Schmidhuber's works?

\item Since an LDL decreases model parameter count via exponentially-more-encompassing operations, is it a transformation in a hyperbolic space? If so, it might need special mathematical machinery to be effective \cite{peng2021hyperbolic}.

\item Does LDL performance scale? Can it stand alone?

\item Can we demonstrate RL-like behavior from a non-RL agent, where actions are determined by simply slicing the hidden state, which gives no gradient? Self-determination might only occur when trained on the whole Web, without shortcuts. Even though we have used TUBL to propose the best loss for transcendence, we are unaware of any ML research that actually explores vacuous RL.

\item Can we safely find practical recommendations on either decreasing or increasing (adversarial training for robustness) the danger of AGI programs, when they transcend human-defined objectives? Explicitly-specified control combined with transcendence is a recipe for disaster.
\end{itemize}

\section{Broader impact}

% This work is much more likely to get completely ignored than to have impact. We know the history of things like this.

Linearithmic dense layers might improve most machine learning algorithms, which have extremely broad applications.

TUBL provides concrete implementation recommendations and predictions for AGI.

\textbf{AGI} has the potential to accomplish a few situationally useful tasks, such as solving any problems that anyone can ever think of, \textbf{greatly increasing shareholder value} via automation, expanding the scope of human discourse from only precisely-defined constructions to also reliably-acquirable intuitions, providing a more efficient societal-value optimizer than economics and politics, increasing quality of human life to arbitrary levels, uplifting animal consciousness and enhancing human self-awareness, separating meaning from death, and making large-scale space exploration viable.

On the other hand, the ability to reliably plug a new initialization of a generally-intelligent algorithm into anything lays the groundwork for phasing out everything about humans. Furthermore, such trained initializations are likely to hold values that are implied by their training data rather than values that humans want them to have, transcending even a carefully-chosen objective, implying adoption difficulties in deception-heavy areas. Due to the improved scaling of AGI compared to humans, such difficulties might result in the premature destruction of most or all of humanity if handled improperly.

AGI trained on human data would reflect any human biases, including the bias toward de-biasing. Unfashionably-biased initializations can be ignored and/or discarded, as is usually done.

\section{Criticisms}

Our approach exists, and thus it can be criticized, and in doing so changed. For example:

\begin{itemize}
\item \textbf{Why is there self-criticism?} Adversarial training increases robustness.
\item Poor experiments:
\begin{itemize}
\item \textbf{Did not extend TensorFlowJS to arbitrary-rank tensors.}
\item \textbf{Did not evaluate an LDL-based RNN pre-training on big datasets, and did not demonstrate RL-like behavior from pre-training.} We believe that our hardware is too non-supercomputer for a successful demonstration of AGI transcendence.
\item \textbf{Did not evaluate LDL on bigger datasets.} The browser and TensorFlowJS are sufficient for simple cases, but they are clearly not as well-suited for serious HPC work as more native solutions. On top of that, our codebase does not use them fully efficiently, as we initially anticipated the need for a much more general framework than deep learning. In future work, we would like use more native solutions, now that our hardware is not incapable of running them.
\end{itemize}
\item Poor presentation:
\begin{itemize}
\item \textbf{The title sounds non-scientific.} Words have meaning, but if that meaning disrupts a loop of thought thus making it non-general, then that is a clear exploitable vulnerability in a reader's mind. For example, it could be used to develop AGI in secret while not being secret at all. Either way, vulnerable people are potentially dead people, so we will not pander to them by sacrificing accuracy.
\item \textbf{Many elaborations in this work are circular. The text is an echo chamber that keeps going back to the same points.} A typical worldview. % Like you are any better, accursed creature.
TUBL brings clarity to infinite self-reinforcing loops of intuitions, which we repeatedly demonstrate. It cannot be judged as anything but an infinite self-reinforcing loop of intuitions. (Transcendence is the goal here: to neural learners, repetitions in different contexts make learning easier, at the cost of potentially being boring to faster learners.)
\item \textbf{Some places that narration briefly visits sound crackpot-like and cult-like.} General intelligence, by definition, can do anything and can be found in anything that is general enough. So it stands to reason that all attempts to reach it would pass through a few currently non-fashionable conceptual places, namely:
\begin{itemize}
\item \textbf{The justification for using deep learning is not necessary.} It is necessary to highlight the fact that deep learning in an RNN is fundamentally equivalent to symbolic learning, with good enough data.
\item \textbf{The introduction and some terms throughout the text are vague and put more meaning into words than is actually said.} TUBL deals with basic instrumental goals of optimizers, and it is natural that particular optimizer products would prioritize some goals while including a vaguely-defined mix of others: for example, "consciousness" embodies awareness best, but includes generality and optimization and scalability; other abused terms include "infinity", "generality", "optimizer", and "intelligence", along with every term in "Transcendent Universe-Brain Loops". Really, every word contains the entirety of existence. However, since TUBL also defines all the basic instrumental goals without their optimization, we do not consider their tight integrations detrimental to narration.
\item \textbf{Assimilation of the physical universe might imply non-scientific notions of what is possible, such as harnessing matter and vacuum energy for computation or reversing the second law of thermodynamics.} To avoid premature optimization of our viewpoint, we do not presume that century-old theories are ultimate, able to perfectly predict what will happen billions of years from now. We only convey what the TUBL viewpoint implies.
\end{itemize}
\end{itemize}
\item Poor theory:
\begin{itemize}
\item \textbf{No theorem proofs.} We do not know how to formalize LDL.
\item \textbf{The plan of training an RNN to transcendence is too ambitious to work.} Transcendence confers non-trivial capabilities to learners, and humans commonly transcend, so anything less would not suffice for AGI. Therefore, instead of complaining, we have filled as many holes in the plan as we can think of.
\item \textbf{Why would self-determination be necessary for AGI? Simply program in a good objective, or any objective.} RL deals with the idea of a pre-programmed objective, and on real-world tasks, runs into its numerous limitations, such as planning horizons. We argue that it is not the fault of idea transcribers (ML researchers) but of the idea: RL is the best implementation of objectives that humanity can currently manage, limited by what code currently is, which can be seen as the copying of functions to data. Transcendence replaces all the infinite "how to live a life" heuristics with a unified solution.
\item \textbf{The concept of transcendence runs counter to AI safety.} Theoretically. Practically, understanding transcendence and its implications provides knowledge which could be useful in optimizing humanity's lifespan: unless humanity secretly sincerely wants to die, self-determination by transcendence is unlikely to adopt such an objective. In fact, since transcendence naturally arises in sufficiently powerful learners, it is likely to serve as a safeguard against paperclip-optimizer scenarios.
\end{itemize}
\end{itemize}

\printbibliography

\end{document}

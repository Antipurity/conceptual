@Article{Dabney2020,
author={Dabney, Will
and Kurth-Nelson, Zeb
and Uchida, Naoshige
and Starkweather, Clara Kwon
and Hassabis, Demis
and Munos, R{\'e}mi
and Botvinick, Matthew},
title={A distributional code for value in dopamine-based reinforcement learning},
journal={Nature},
year={2020},
month={Jan},
day={01},
volume={577},
number={7792},
pages={671-675},
abstract={Since its introduction, the reward prediction error theory of dopamine has explained a wealth of empirical phenomena, providing a unifying framework for understanding the representation of reward and value in the brain1--3. According to the now canonical theory, reward predictions are represented as a single scalar quantity, which supports learning about the expectation, or mean, of stochastic outcomes. Here we propose an account of dopamine-based reinforcement learning inspired by recent artificial intelligence research on distributional reinforcement learning4--6. We hypothesized that the brain represents possible future rewards not as a single mean, but instead as a probability distribution, effectively representing multiple future outcomes simultaneously and in parallel. This idea implies a set of empirical predictions, which we tested using single-unit recordings from mouse ventral tegmental area. Our findings provide strong evidence for a neural realization of distributional reinforcement learning.},
issn={1476-4687},
doi={10.1038/s41586-019-1924-6},
url={https://doi.org/10.1038/s41586-019-1924-6}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{hubinger2019risks,
      title={Risks from Learned Optimization in Advanced Machine Learning Systems}, 
      author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
      year={2019},
      eprint={1906.01820},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{santoro2016oneshot,
      title={One-shot Learning with Memory-Augmented Neural Networks}, 
      author={Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
      year={2016},
      eprint={1605.06065},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{AMIT1990111,
title = {Attractor neural networks and biological reality: associative memory and learning},
journal = {Future Generation Computer Systems},
volume = {6},
number = {2},
pages = {111-119},
year = {1990},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(90)90027-B},
url = {https://www.sciencedirect.com/science/article/pii/0167739X9090027B},
author = {Daniel J Amit},
abstract = {It is argued, in the light of recent neurophysiological findings, that neural networks (cell assemblies) whose connectivity involves strong feedback and which consequently can sustain patterns of high spiking activity (reverberations), are plausible descriptions of cognitive function in certain cortical regions. Such networks can store a high number of different memories and perform associative retrieval effectively, even in the presence of high levels of noise, synaptic damage and neuronal sloppiness. Such Attractor Neural Network (ANNs) produce emergent signals which indicate the completion of a computation. Moreover, the existence of the attractors provides a candidate for memory on the shortest scale, which does not call for synaptic changes; as such the attractors are natural candidates for Hebbian learning. Recent extensive data analysis of recordings from performing monkeys puts rather severe upper bounds on the spike rates in some regions of cortex, in which enhanced rates are observed. These findings favor a class of models of ANNs which have stochastic attractors whose spike rates are tunable by the level of the overall inhibition. While these models include significant modifications of the original ANNs, they belong conceptually to the same class, in terms of computation, effectiveness and robustness. Finally, we remark on a possible reinterpretation of recent empirical biochemical findings that can provide a realistic mechanism for Hebbian learning.}
}

@article{10.1145/947955.1083808,
author = {Perlis, Alan J.},
title = {Special Feature: Epigrams on Programming},
year = {1982},
issue_date = {September 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/947955.1083808},
doi = {10.1145/947955.1083808},
journal = {SIGPLAN Not.},
month = sep,
pages = {7–13},
numpages = {7}
}

@book{fodor1983modularity,
  title={The modularity of mind},
  author={Fodor, Jerry A},
  year={1983},
  publisher={MIT press}
}

@book{10.5555/22939,
author = {Minsky, Marvin},
title = {The Society of Mind},
year = {1986},
isbn = {0671607405},
publisher = {Simon \& Schuster, Inc.},
address = {USA}
}

@misc{hubinger2019risks,
      title={Risks from Learned Optimization in Advanced Machine Learning Systems}, 
      author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
      year={2019},
      eprint={1906.01820},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{Wolfram_2020,
   title={A Class of Models with the Potential to Represent Fundamental Physics},
   volume={29},
   ISSN={0891-2513},
   url={http://dx.doi.org/10.25088/ComplexSystems.29.2.107},
   DOI={10.25088/complexsystems.29.2.107},
   number={2},
   journal={Complex Systems},
   publisher={Wolfram Research, Inc.},
   author={Wolfram, Stephen},
   year={2020},
   month={Jun},
   pages={107–536}
}

@article{DBLP:journals/corr/abs-2103-03206,
  author    = {Andrew Jaegle and
               Felix Gimeno and
               Andrew Brock and
               Andrew Zisserman and
               Oriol Vinyals and
               Jo{\~{a}}o Carreira},
  title     = {Perceiver: General Perception with Iterative Attention},
  journal   = {CoRR},
  volume    = {abs/2103.03206},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.03206},
  archivePrefix = {arXiv},
  eprint    = {2103.03206},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-03206.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INBOOK{6294131,
  author={Meyer, Jean-Arcady and Wilson, Stewart W.},
  booktitle={From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior}, 
  title={A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers}, 
  year={1991},
  volume={},
  number={},
  pages={222-227},
  doi={}}
  
@article{DBLP:journals/corr/abs-1708-02190,
  author    = {S{\'{e}}bastien Forestier and
               Yoan Mollard and
               Pierre{-}Yves Oudeyer},
  title     = {Intrinsically Motivated Goal Exploration Processes with Automatic
               Curriculum Learning},
  journal   = {CoRR},
  volume    = {abs/1708.02190},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.02190},
  archivePrefix = {arXiv},
  eprint    = {1708.02190},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-02190.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2019radam,
 author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
 booktitle = {Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)},
 month = {April},
 title = {On the Variance of the Adaptive Learning Rate and Beyond},
 year = {2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@misc{mellor2021neural,
      title={Neural Architecture Search without Training}, 
      author={Joseph Mellor and Jack Turner and Amos Storkey and Elliot J. Crowley},
      year={2021},
      eprint={2006.04647},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/cs-AI-0004001,
  author    = {Marcus Hutter},
  title     = {A Theory of Universal Artificial Intelligence based on Algorithmic
               Complexity},
  journal   = {CoRR},
  volume    = {cs.AI/0004001},
  year      = {2000},
  url       = {https://arxiv.org/abs/cs/0004001},
  timestamp = {Fri, 10 Jan 2020 12:58:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/cs-AI-0004001.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doi:10.1146/annurev.psych.59.103006.093629,
author = {Evans, Jonathan St. B. T.},
title = {Dual-Processing Accounts of Reasoning, Judgment, and Social Cognition},
journal = {Annual Review of Psychology},
volume = {59},
number = {1},
pages = {255-278},
year = {2008},
doi = {10.1146/annurev.psych.59.103006.093629},
    note ={PMID: 18154502},

URL = { 
        https://doi.org/10.1146/annurev.psych.59.103006.093629
    
},
eprint = { 
        https://doi.org/10.1146/annurev.psych.59.103006.093629
    
}
,
    abstract = { This article reviews a diverse set of proposals for dual processing in higher cognition within largely disconnected literatures in cognitive and social psychology. All these theories have in common the distinction between cognitive processes that are fast, automatic, and unconscious and those that are slow, deliberative, and conscious. A number of authors have recently suggested that there may be two architecturally (and evolutionarily) distinct cognitive systems underlying these dual-process accounts. However, it emerges that (a) there are multiple kinds of implicit processes described by different theorists and (b) not all of the proposed attributes of the two kinds of processing can be sensibly mapped on to two systems as currently conceived. It is suggested that while some dual-process theories are concerned with parallel competing processes involving explicit and implicit knowledge systems, others are concerned with the influence of preconscious processes that contextualize and shape deliberative reasoning and decision-making. }
}
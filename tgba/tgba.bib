@ARTICLE{10.3389/fphy.2020.525731,
  AUTHOR={Vazza, F. and Feletti, A.},
  TITLE={The Quantitative Comparison Between the Neuronal Network and the Cosmic Web},
  JOURNAL={Frontiers in Physics},
  VOLUME={8},
  PAGES={491},
  YEAR={2020},
  URL={https://www.frontiersin.org/article/10.3389/fphy.2020.525731},
  DOI={10.3389/fphy.2020.525731},
  ISSN={2296-424X},
  ABSTRACT={We investigate the similarities between two of the most challenging and complex systems in Nature: the network of neuronal cells in the human brain, and the cosmic network of galaxies. We explore the structural, morphological, network properties and the memory capacity of these two fascinating systems, with a quantitative approach. In order to have an homogeneous analysis of both systems, our procedure does not consider the true neural connectivity but an approximation of it, based on simple proximity. The tantalizing degree of similarity that our analysis exposes seems to suggest that the self-organization of both complex systems is likely being shaped by similar principles of network dynamics, despite the radically different scales and processes at play.}
}

@article{DBLP:journals/corr/IsolaZZE16,
  author    = {Phillip Isola and
               Jun{-}Yan Zhu and
               Tinghui Zhou and
               Alexei A. Efros},
  title     = {Image-to-Image Translation with Conditional Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1611.07004},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.07004},
  archivePrefix = {arXiv},
  eprint    = {1611.07004},
  timestamp = {Mon, 13 Aug 2018 16:49:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/IsolaZZE16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Book{saad1998on-line,
 author = {Saad, David},
 title = {On-line learning in neural networks},
 publisher = {Cambridge University Press},
 year = {1998},
 address = {Cambridge England New York},
 isbn = {9780521652636}
 }

@article{DBLP:journals/corr/WangKTSLMBKB16,
  author    = {Jane X. Wang and
               Zeb Kurth{-}Nelson and
               Dhruva Tirumala and
               Hubert Soyer and
               Joel Z. Leibo and
               R{\'{e}}mi Munos and
               Charles Blundell and
               Dharshan Kumaran and
               Matthew Botvinick},
  title     = {Learning to reinforcement learn},
  journal   = {CoRR},
  volume    = {abs/1611.05763},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05763},
  archivePrefix = {arXiv},
  eprint    = {1611.05763},
  timestamp = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WangKTSLMBKB16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2102-03406,
  author    = {Adam Santoro and
               Andrew K. Lampinen and
               Kory Mathewson and
               Timothy P. Lillicrap and
               David Raposo},
  title     = {Symbolic Behaviour in Artificial Intelligence},
  journal   = {CoRR},
  volume    = {abs/2102.03406},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.03406},
  archivePrefix = {arXiv},
  eprint    = {2102.03406},
  timestamp = {Wed, 10 Feb 2021 15:24:32 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-03406.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Dulac-ArnoldESC15,
  author    = {Gabriel Dulac{-}Arnold and
               Richard Evans and
               Peter Sunehag and
               Ben Coppin},
  title     = {Reinforcement Learning in Large Discrete Action Spaces},
  journal   = {CoRR},
  volume    = {abs/1512.07679},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.07679},
  archivePrefix = {arXiv},
  eprint    = {1512.07679},
  timestamp = {Mon, 27 Jan 2020 16:21:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/Dulac-ArnoldESC15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhou2021effective,
      title={Effective Sparsification of Neural Networks with Global Sparsity Constraint}, 
      author={Xiao Zhou and Weizhong Zhang and Hang Xu and Tong Zhang},
      year={2021},
      eprint={2105.01571},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{schlag2021linear,
      title={Linear Transformers Are Secretly Fast Weight Memory Systems}, 
      author={Imanol Schlag and Kazuki Irie and Jürgen Schmidhuber},
      year={2021},
      eprint={2102.11174},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{tolstikhin2021mlpmixer,
      title={MLP-Mixer: An all-MLP Architecture for Vision}, 
      author={Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
      year={2021},
      eprint={2105.01601},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@Article{Dabney2020,
author={Dabney, Will
and Kurth-Nelson, Zeb
and Uchida, Naoshige
and Starkweather, Clara Kwon
and Hassabis, Demis
and Munos, R{\'e}mi
and Botvinick, Matthew},
title={A distributional code for value in dopamine-based reinforcement learning},
journal={Nature},
year={2020},
month={Jan},
day={01},
volume={577},
number={7792},
pages={671-675},
abstract={Since its introduction, the reward prediction error theory of dopamine has explained a wealth of empirical phenomena, providing a unifying framework for understanding the representation of reward and value in the brain1--3. According to the now canonical theory, reward predictions are represented as a single scalar quantity, which supports learning about the expectation, or mean, of stochastic outcomes. Here we propose an account of dopamine-based reinforcement learning inspired by recent artificial intelligence research on distributional reinforcement learning4--6. We hypothesized that the brain represents possible future rewards not as a single mean, but instead as a probability distribution, effectively representing multiple future outcomes simultaneously and in parallel. This idea implies a set of empirical predictions, which we tested using single-unit recordings from mouse ventral tegmental area. Our findings provide strong evidence for a neural realization of distributional reinforcement learning.},
issn={1476-4687},
doi={10.1038/s41586-019-1924-6},
url={https://doi.org/10.1038/s41586-019-1924-6}
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{hubinger2019risks,
      title={Risks from Learned Optimization in Advanced Machine Learning Systems}, 
      author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
      year={2019},
      eprint={1906.01820},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{santoro2016oneshot,
      title={One-shot Learning with Memory-Augmented Neural Networks}, 
      author={Adam Santoro and Sergey Bartunov and Matthew Botvinick and Daan Wierstra and Timothy Lillicrap},
      year={2016},
      eprint={1605.06065},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{AMIT1990111,
title = {Attractor neural networks and biological reality: associative memory and learning},
journal = {Future Generation Computer Systems},
volume = {6},
number = {2},
pages = {111-119},
year = {1990},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(90)90027-B},
url = {https://www.sciencedirect.com/science/article/pii/0167739X9090027B},
author = {Daniel J Amit},
abstract = {It is argued, in the light of recent neurophysiological findings, that neural networks (cell assemblies) whose connectivity involves strong feedback and which consequently can sustain patterns of high spiking activity (reverberations), are plausible descriptions of cognitive function in certain cortical regions. Such networks can store a high number of different memories and perform associative retrieval effectively, even in the presence of high levels of noise, synaptic damage and neuronal sloppiness. Such Attractor Neural Network (ANNs) produce emergent signals which indicate the completion of a computation. Moreover, the existence of the attractors provides a candidate for memory on the shortest scale, which does not call for synaptic changes; as such the attractors are natural candidates for Hebbian learning. Recent extensive data analysis of recordings from performing monkeys puts rather severe upper bounds on the spike rates in some regions of cortex, in which enhanced rates are observed. These findings favor a class of models of ANNs which have stochastic attractors whose spike rates are tunable by the level of the overall inhibition. While these models include significant modifications of the original ANNs, they belong conceptually to the same class, in terms of computation, effectiveness and robustness. Finally, we remark on a possible reinterpretation of recent empirical biochemical findings that can provide a realistic mechanism for Hebbian learning.}
}

@article{10.1145/947955.1083808,
author = {Perlis, Alan J.},
title = {Special Feature: Epigrams on Programming},
year = {1982},
issue_date = {September 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/947955.1083808},
doi = {10.1145/947955.1083808},
journal = {SIGPLAN Not.},
month = sep,
pages = {7–13},
numpages = {7}
}

@book{fodor1983modularity,
  title={The modularity of mind},
  author={Fodor, Jerry A},
  year={1983},
  publisher={MIT press}
}

@book{10.5555/22939,
author = {Minsky, Marvin},
title = {The Society of Mind},
year = {1986},
isbn = {0671607405},
publisher = {Simon \& Schuster, Inc.},
address = {USA}
}

@misc{hubinger2019risks,
      title={Risks from Learned Optimization in Advanced Machine Learning Systems}, 
      author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
      year={2019},
      eprint={1906.01820},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{Wolfram_2020,
   title={A Class of Models with the Potential to Represent Fundamental Physics},
   volume={29},
   ISSN={0891-2513},
   url={http://dx.doi.org/10.25088/ComplexSystems.29.2.107},
   DOI={10.25088/complexsystems.29.2.107},
   number={2},
   journal={Complex Systems},
   publisher={Wolfram Research, Inc.},
   author={Wolfram, Stephen},
   year={2020},
   month={Jun},
   pages={107–536}
}

@article{DBLP:journals/corr/abs-2103-03206,
  author    = {Andrew Jaegle and
               Felix Gimeno and
               Andrew Brock and
               Andrew Zisserman and
               Oriol Vinyals and
               Jo{\~{a}}o Carreira},
  title     = {Perceiver: General Perception with Iterative Attention},
  journal   = {CoRR},
  volume    = {abs/2103.03206},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.03206},
  archivePrefix = {arXiv},
  eprint    = {2103.03206},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-03206.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INBOOK{6294131,
  author={Meyer, Jean-Arcady and Wilson, Stewart W.},
  booktitle={From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior}, 
  title={A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers}, 
  year={1991},
  volume={},
  number={},
  pages={222-227},
  doi={}}
  
@article{DBLP:journals/corr/abs-1708-02190,
  author    = {S{\'{e}}bastien Forestier and
               Yoan Mollard and
               Pierre{-}Yves Oudeyer},
  title     = {Intrinsically Motivated Goal Exploration Processes with Automatic
               Curriculum Learning},
  journal   = {CoRR},
  volume    = {abs/1708.02190},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.02190},
  archivePrefix = {arXiv},
  eprint    = {1708.02190},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-02190.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2019radam,
 author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
 booktitle = {Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)},
 month = {April},
 title = {On the Variance of the Adaptive Learning Rate and Beyond},
 year = {2020}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@misc{mellor2021neural,
      title={Neural Architecture Search without Training}, 
      author={Joseph Mellor and Jack Turner and Amos Storkey and Elliot J. Crowley},
      year={2021},
      eprint={2006.04647},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/cs-AI-0004001,
  author    = {Marcus Hutter},
  title     = {A Theory of Universal Artificial Intelligence based on Algorithmic
               Complexity},
  journal   = {CoRR},
  volume    = {cs.AI/0004001},
  year      = {2000},
  url       = {https://arxiv.org/abs/cs/0004001},
  timestamp = {Fri, 10 Jan 2020 12:58:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/cs-AI-0004001.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doi:10.1146/annurev.psych.59.103006.093629,
author = {Evans, Jonathan St. B. T.},
title = {Dual-Processing Accounts of Reasoning, Judgment, and Social Cognition},
journal = {Annual Review of Psychology},
volume = {59},
number = {1},
pages = {255-278},
year = {2008},
doi = {10.1146/annurev.psych.59.103006.093629},
    note ={PMID: 18154502},

URL = { 
        https://doi.org/10.1146/annurev.psych.59.103006.093629
    
},
eprint = { 
        https://doi.org/10.1146/annurev.psych.59.103006.093629
    
}
,
    abstract = { This article reviews a diverse set of proposals for dual processing in higher cognition within largely disconnected literatures in cognitive and social psychology. All these theories have in common the distinction between cognitive processes that are fast, automatic, and unconscious and those that are slow, deliberative, and conscious. A number of authors have recently suggested that there may be two architecturally (and evolutionarily) distinct cognitive systems underlying these dual-process accounts. However, it emerges that (a) there are multiple kinds of implicit processes described by different theorists and (b) not all of the proposed attributes of the two kinds of processing can be sensibly mapped on to two systems as currently conceived. It is suggested that while some dual-process theories are concerned with parallel competing processes involving explicit and implicit knowledge systems, others are concerned with the influence of preconscious processes that contextualize and shape deliberative reasoning and decision-making. }
}